{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"bookacle","text":"<p>Answer queries on complex PDF documents using RAPTOR-based RAG.</p>"},{"location":"#raptor-overview","title":"RAPTOR Overview","text":"<p>RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) is a RAG technique designed to work with large documents in a limited context. From the abstract:</p> <p>Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.</p> <p>It builds a hierarchial tree structure on the documents and queries the tree to retrieve relevant context. The idea is that the upper layers of the tree represent a more holistic understanding of the documents and as we go down the tree, the understanding becomes more granular until we reach the leaf nodes, where the actual text from the documents resides.</p> <p>This holistic understanding is achieved as follows:</p> <ul> <li>The documents are split into small chunks.</li> <li>These chunks are embedded using an embedding model and become the leaf nodes of the tree structure.</li> <li>A clustering algorithm is used to cluster these leaf nodes.</li> <li>For each cluster:<ul> <li>The texts of the nodes in the cluster are concatenated.</li> <li>A summary of the concatenated text is generated using a model.</li> <li>The summary is embedded using the same embedding model and becomes a node in the next layer.</li> <li>The children of this node are the nodes in the cluster.</li> </ul> </li> <li>This process is repeated until no further layers can be made.</li> </ul> <p>Since each subsequent layer is a summary of some nodes in the previous layer, it represents a holistic understanding of those nodes, helping build a holistic understanding of the overall documents as we go up the tree.</p> <p>For more details on RAPTOR, refer to the paper: https://arxiv.org/abs/2401.18059.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Everything is a Protocol, allowing for convenient extensibility.</li> <li>Completely local out of the box - no OpenAI key required.</li> <li>Sensible default implementations for embeddings models, summarization models and question-answering models. See Models for more details.</li> <li>Use custom embedding models, summarization models and question-answering easily - just implement the protocol. See Models for more details.</li> <li>Load your PDFs as text or markdown using the provided loaders or implement your own. See Loaders for more details.</li> <li>Use any tokenizer as long as it follows the <code>TokenizerLike</code> protocol. See Tokenizers for more details.</li> <li>Split your documents into chunks easily using the provided splitters or implement your own. See Splitters for more details.</li> <li>Customize the default RAPTOR-tree building methodology by implementing your own clustering logic. See Clustering for more details.</li> <li>Implement your own RAPTOR-tree building methodology by implementing the <code>TreeBuilderLike</code> protocol. See Building RAPTOR Tree for more details.</li> <li>Use a terminal-based chat to chat with your documents. See Command-Line Interface for more details.</li> <li>Define configuration for <code>bookacle</code> using TOML files and use them throughout your application. See Configuration for more details.</li> </ul>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2024 Malay Agarwal\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n</code></pre>"},{"location":"reference/","title":"bookacle","text":""},{"location":"reference/#bookacle","title":"bookacle","text":""},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> bookacle<ul> <li> chat</li> <li> cli</li> <li> conf<ul> <li> config</li> </ul> </li> <li> document</li> <li> loaders</li> <li> models<ul> <li> embedding</li> <li> message</li> <li> qa</li> <li> summarization</li> </ul> </li> <li> splitters</li> <li> tokenizer</li> <li> tree<ul> <li> builder</li> <li> clustering</li> <li> config</li> <li> retriever</li> <li> structures</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/chat/","title":"bookacle.chat","text":""},{"location":"reference/chat/#bookacle.chat","title":"chat","text":"<p>Chat module for conversing with a RAPTOR RAG-based LLM in the terminal.</p>"},{"location":"reference/chat/#bookacle.chat.Chat","title":"Chat","text":"<pre><code>Chat(\n    retriever: RetrieverLike,\n    qa_model: QAModelLike,\n    console: Console,\n    history_file: str = \".bookacle-chat-history.txt\",\n    user_avatar: str = \"\ud83d\udc64\",\n)\n</code></pre> <p>A terminal-based chat interface for interacting with a RAPTOR RAG-based LLM.</p> <p>Attributes:</p> <ul> <li> <code>retriever</code>               (<code>RetrieverLike</code>)           \u2013            <p>Retriever to use for retrieving relevant context.</p> </li> <li> <code>qa_model</code>               (<code>QAModelLike</code>)           \u2013            <p>QA model to use for answering questions.</p> </li> <li> <code>console</code>               (<code>Console</code>)           \u2013            <p>Rich Console to use for displaying messages.</p> </li> <li> <code>history_file</code>               (<code>str</code>)           \u2013            <p>File to store chat history.</p> </li> <li> <code>user_avatar</code>               (<code>str</code>)           \u2013            <p>Avatar to use for the user in the chat UI.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>retriever</code>               (<code>RetrieverLike</code>)           \u2013            <p>Retriever to use for retrieving relevant context.</p> </li> <li> <code>qa_model</code>               (<code>QAModelLike</code>)           \u2013            <p>QA model to use for answering questions.</p> </li> <li> <code>console</code>               (<code>Console</code>)           \u2013            <p>Rich Console to use for displaying messages.</p> </li> <li> <code>history_file</code>               (<code>str</code>, default:                   <code>'.bookacle-chat-history.txt'</code> )           \u2013            <p>File to store chat history. The file is created in the home directory.</p> </li> <li> <code>user_avatar</code>               (<code>str</code>, default:                   <code>'\ud83d\udc64'</code> )           \u2013            <p>Avatar to use for the user in the chat UI.</p> </li> </ul>"},{"location":"reference/chat/#bookacle.chat.Chat.display_ai_msg_stream","title":"display_ai_msg_stream","text":"<pre><code>display_ai_msg_stream(messages: Iterator[Message]) -&gt; str\n</code></pre> <p>Display an AI message stream in the chat UI.</p> <p>Parameters:</p> <ul> <li> <code>messages</code>               (<code>Iterator[Message]</code>)           \u2013            <p>Stream of AI messages to display.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The complete message as a string.</p> </li> </ul>"},{"location":"reference/chat/#bookacle.chat.Chat.invoke_qa_model","title":"invoke_qa_model","text":"<pre><code>invoke_qa_model(\n    tree: Tree,\n    question: str,\n    history: list[Message] | None = None,\n    stream: bool = True,\n    *args,\n    **kwargs\n) -&gt; Message\n</code></pre> <p>Invoke the QA model to answer a question.</p> <p>Parameters:</p> <ul> <li> <code>tree</code>               (<code>Tree</code>)           \u2013            <p>RAPTOR tree that should be used for RAG.</p> </li> <li> <code>question</code>               (<code>str</code>)           \u2013            <p>The question to answer.</p> </li> <li> <code>history</code>               (<code>list[Message] | None</code>, default:                   <code>None</code> )           \u2013            <p>Chat history.</p> </li> <li> <code>stream</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to stream the AI response.</p> </li> <li> <code>**args</code>               (<code>tuple[Any]</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments to pass to the retriever.</p> </li> <li> <code>**kwargs</code>               (<code>dict[str, Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to the retriever.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Message</code>           \u2013            <p>The response from the QA model.</p> </li> </ul>"},{"location":"reference/chat/#bookacle.chat.Chat.run","title":"run","text":"<pre><code>run(\n    tree: Tree,\n    initial_chat_message: str = \"\",\n    system_prompt: str = \"\",\n    stream: bool = True,\n    *args,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Run the chat interface.</p> <p>Parameters:</p> <ul> <li> <code>tree</code>               (<code>Tree</code>)           \u2013            <p>RAPTOR tree that should be used for RAG.</p> </li> <li> <code>initial_chat_message</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Initial message to display in the chat.</p> </li> <li> <code>system_prompt</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>System prompt that should be used for the QA model.</p> </li> <li> <code>stream</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to stream the AI response.</p> </li> <li> <code>*args</code>               (<code>tuple[Any]</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments to pass to the retriever.</p> </li> <li> <code>**kwargs</code>               (<code>dict[str, Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to the retriever.</p> </li> </ul>"},{"location":"reference/cli/","title":"bookacle.cli","text":""},{"location":"reference/cli/#bookacle.cli","title":"cli","text":""},{"location":"reference/document/","title":"bookacle.document","text":""},{"location":"reference/document/#bookacle.document","title":"document","text":"<p>This module defines the document structure used throughout the package.</p>"},{"location":"reference/document/#bookacle.document.Document","title":"Document","text":"<p>               Bases: <code>TypedDict</code></p> <p>A TypedDict that represents a page in a PDF file.</p> <p>Attributes:</p> <ul> <li> <code>page_content</code>               (<code>str</code>)           \u2013            <p>The text content of the page.</p> </li> <li> <code>metadata</code>               (<code>NotRequired[dict[str, Any]]</code>)           \u2013            <p>Additional metadata about the page.</p> </li> </ul>"},{"location":"reference/loaders/","title":"bookacle.loaders","text":""},{"location":"reference/loaders/#bookacle.loaders","title":"loaders","text":"<p>This module defines functions for loading PDF documents and some utilities to manage loaders.</p>"},{"location":"reference/loaders/#bookacle.loaders.LOADER_MANAGER","title":"LOADER_MANAGER  <code>module-attribute</code>","text":"<pre><code>LOADER_MANAGER = LoaderManager()\n</code></pre> <p>Default loader manager.</p>"},{"location":"reference/loaders/#bookacle.loaders.LoaderLike","title":"LoaderLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that all document loaders should follow.</p>"},{"location":"reference/loaders/#bookacle.loaders.LoaderLike.__call__","title":"__call__","text":"<pre><code>__call__(\n    file_path: str,\n    start_page: int = 0,\n    end_page: int | None = None,\n) -&gt; list[Document]\n</code></pre> <p>Load a PDF document.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>The path to the PDF file.</p> </li> <li> <code>start_page</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The starting (0-based) page number in the PDF to begin reading from.</p> </li> <li> <code>end_page</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The ending (0-based) page number to stop reading at (non-inclusive).       When <code>None</code>, all pages in the PDF are read.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Document]</code>           \u2013            <p>Pages in the file.</p> </li> </ul>"},{"location":"reference/loaders/#bookacle.loaders.LoaderManager","title":"LoaderManager","text":"<p>               Bases: <code>UserDict[str, LoaderLike]</code></p> <p>Manager to maintain registry of all document loaders.</p> <p>It behaves like a dictionary, where each document loader is registered to a name.</p> <p>Examples:</p> <pre><code>from bookacle.loaders import LoaderManager, register_loader\nfrom langchain_core.documents import Document\n\nmanager = LoaderManager()\n\n@register_loader(name=\"custom_loader\", manager=manager)\ndef doc_loader(file_path: str, start_page: int = 0, end_page: int | None = None) -&gt; list[Document]:\n    ...\n\nprint(manager[\"custom_loader\"] is doc_loader)\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"reference/loaders/#bookacle.loaders.LoaderManager.enum","title":"enum  <code>property</code>","text":"<pre><code>enum: Enum\n</code></pre> <p>Obtain the names of the document loaders as an Enum.</p> <p>Useful in the CLI for <code>--help</code>.</p>"},{"location":"reference/loaders/#bookacle.loaders.register_loader","title":"register_loader","text":"<pre><code>register_loader(\n    name: str, manager: LoaderManager | None = None\n) -&gt; Callable[[LoaderLike], LoaderLike]\n</code></pre> <p>A decorator that registers a loader function with the loader manager.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name to map the loader function to.</p> </li> <li> <code>manager</code>               (<code>LoaderManager | None</code>, default:                   <code>None</code> )           \u2013            <p>The manager to register the function with.      If <code>None</code>, <code>LOADER_MANAGER</code> is used.</p> </li> </ul>"},{"location":"reference/loaders/#bookacle.loaders.pymupdf4llm_loader","title":"pymupdf4llm_loader","text":"<pre><code>pymupdf4llm_loader(\n    file_path: str,\n    start_page: int = 0,\n    end_page: int | None = None,\n) -&gt; list[Document]\n</code></pre> <p>Document loader which uses <code>pymupdf4llm</code> to load the PDF as Markdown.</p> <p>Can be accessed using the name <code>'pymupdf4llm'</code> via the default loader manager.</p> <p>It implements the LoaderLike protocol.</p>"},{"location":"reference/loaders/#bookacle.loaders.pymupdf_loader","title":"pymupdf_loader","text":"<pre><code>pymupdf_loader(\n    file_path: str,\n    start_page: int = 0,\n    end_page: int | None = None,\n) -&gt; list[Document]\n</code></pre> <p>Document loader which uses <code>pymupdf</code> to load the PDF as text.</p> <p>Can be accessed using the name <code>'pymupdf'</code> via the default loader manager.</p> <p>It implements the LoaderLike protocol.</p>"},{"location":"reference/splitters/","title":"bookacle.splitters","text":""},{"location":"reference/splitters/#bookacle.splitters","title":"splitters","text":"<p>This module implements document splitters for document chunking.</p>"},{"location":"reference/splitters/#bookacle.splitters.DocumentSplitterLike","title":"DocumentSplitterLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the methods that a document splitter should implement.</p>"},{"location":"reference/splitters/#bookacle.splitters.DocumentSplitterLike.__call__","title":"__call__","text":"<pre><code>__call__(\n    documents: list[Document],\n    chunk_size: int = 100,\n    chunk_overlap: int = 0,\n) -&gt; list[Document]\n</code></pre> <p>Split a list of documents into smaller chunks.</p> <p>Parameters:</p> <ul> <li> <code>documents</code>               (<code>list[Document]</code>)           \u2013            <p>The list of documents to be split.</p> </li> <li> <code>chunk_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The size of each chunk.</p> </li> <li> <code>chunk_overlap</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The overlap between consecutive chunks.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Document]</code>           \u2013            <p>The list of documents after splitting into chunks.</p> </li> </ul>"},{"location":"reference/splitters/#bookacle.splitters.HuggingFaceTextSplitter","title":"HuggingFaceTextSplitter","text":"<pre><code>HuggingFaceTextSplitter(\n    tokenizer: PreTrainedTokenizerBase,\n    separators: list[str] | None = None,\n)\n</code></pre> <p>Text-based document splitter which uses a HuggingFace tokenizer to calculate length when splitting.</p> <p>It uses Langchain\u2019s RecursiveCharacterTextSplitter and expects the list of documents to be in plain text.</p> <p>It implements the DocumentSplitterLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>tokenizer</code>           \u2013            <p>The HuggingFace tokenizer to use for calculating length.</p> </li> <li> <code>separators</code>           \u2013            <p>The list of separators to use for splitting the document.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>PreTrainedTokenizerBase</code>)           \u2013            <p>The HuggingFace tokenizer to use for calculating length.</p> </li> <li> <code>separators</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>The list of separators to use.         When <code>None</code>, the default separators are used: <code>[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\"]</code>.</p> </li> </ul>"},{"location":"reference/splitters/#bookacle.splitters.HuggingFaceMarkdownSplitter","title":"HuggingFaceMarkdownSplitter","text":"<pre><code>HuggingFaceMarkdownSplitter(\n    tokenizer: PreTrainedTokenizerBase,\n)\n</code></pre> <p>Markdown-based document splitter which uses a HuggingFace tokenizer to calculate length of chunks when splitting.</p> <p>It uses Langchain\u2019s MarkdownTextSplitter and expects the list of documents to be in Markdown.</p> <p>It implements the DocumentSplitterLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>tokenizer</code>           \u2013            <p>The HuggingFace tokenizer to use for calculating length.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>PreTrainedTokenizerBase</code>)           \u2013            <p>The HuggingFace tokenizer to use to calculate length.</p> </li> </ul>"},{"location":"reference/splitters/#bookacle.splitters.RaptorSplitter","title":"RaptorSplitter","text":"<pre><code>RaptorSplitter(\n    tokenizer: TokenizerLike,\n    *,\n    separators: list[str] | None = None\n)\n</code></pre> <p>Document splitter which implements the chunking technique as defined in the RAPTOR paper.</p> <p>It expects a tokenizer which implements the TokenizerLike protocol to calculate the length of chunks.</p> <p>For more details, see: https://github.com/parthsarthi03/raptor/blob/7da1d48a7e1d7dec61a63c9d9aae84e2dfaa5767/raptor/utils.py#L22.</p> <p>It implements the DocumentSplitterLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>tokenizer</code>           \u2013            <p>Tokenizer to use for calculating chunk lengths.</p> </li> <li> <code>separators</code>           \u2013            <p>The list of separators to use for splitting the document.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>TokenizerLike</code>)           \u2013            <p>Tokenizer to use for calculating chunk lengths.</p> </li> <li> <code>separators</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>The list of separators to use.         When <code>None</code>, the default separators are used: <code>[\".\", \"!\", \"?\", \"\\n\"]</code>.</p> </li> </ul>"},{"location":"reference/splitters/#bookacle.splitters.RaptorSplitter.split_single_document","title":"split_single_document","text":"<pre><code>split_single_document(\n    document: Document, chunk_size: int, chunk_overlap: int\n) -&gt; list[Document]\n</code></pre> <p>Split a single document into chunks.</p> <p>Parameters:</p> <ul> <li> <code>document</code>               (<code>Document</code>)           \u2013            <p>Document to split into chunks.</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>Maximum size of each chunk.</p> </li> <li> <code>chunk_overlap</code>               (<code>int</code>)           \u2013            <p>Overlap between each chunk.</p> </li> </ul>"},{"location":"reference/tokenizer/","title":"bookacle.tokenizer","text":""},{"location":"reference/tokenizer/#bookacle.tokenizer","title":"tokenizer","text":""},{"location":"reference/tokenizer/#bookacle.tokenizer.TokenizerLike","title":"TokenizerLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that all tokenizers should follow.</p>"},{"location":"reference/tokenizer/#bookacle.tokenizer.TokenizerLike.encode","title":"encode","text":"<pre><code>encode(*args, **kwargs) -&gt; list[int]\n</code></pre> <p>Tokenize the input text into a list of integers.</p> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>Tokenized input.</p> </li> </ul>"},{"location":"reference/conf/","title":"bookacle.conf","text":""},{"location":"reference/conf/#bookacle.conf","title":"conf","text":""},{"location":"reference/conf/config/","title":"bookacle.conf.config","text":""},{"location":"reference/conf/config/#bookacle.conf.config","title":"config","text":"<p>This module manages loading and validating configuration settings for the application from TOML files.</p> <p>It also provides utility functions for importing classes and instantiating objects based on their dotted paths.</p>"},{"location":"reference/conf/config/#bookacle.conf.config.settings","title":"settings  <code>module-attribute</code>","text":"<pre><code>settings: LazySettings = Dynaconf(\n    envvar_prefix=\"BOOKACLE\",\n    root_path=ROOT_PATH,\n    settings_files=[\"settings.toml\", \"prompts.toml\"],\n)\n</code></pre> <p>Settings for the application.</p> <p>The following validators are registered on the settings object.</p> <pre><code>from dynaconf import Validator\n\n[\nValidator(\n    \"custom_loaders_dir\",\n    \"clustering_func\",\n    \"stream_output\",\n    \"embedding_model.model_class\",\n    \"embedding_model.model_arguments\",\n    \"summarization_model.model_class\",\n    \"summarization_model.model_arguments\",\n    \"document_splitter.splitter_class\",\n    \"document_splitter.splitter_arguments\",\n    \"retriever.retriever_class\",\n    \"retriever.retriever_arguments\",\n    \"clustering_backend.backend_class\",\n    \"clustering_backend.backend_arguments\",\n    \"qa_model.model_class\",\n    \"qa_model.model_arguments\",\n    must_exist=True,\n),\nValidator(\n    \"embedding_model\",\n    cast=lambda value: cast_class_path_to_instance(\n        value[\"model_class\"], value[\"model_arguments\"]\n    ),\n),\nValidator(\n    \"summarization_model\",\n    cast=lambda value: cast_class_path_to_instance(\n        class_path=value[\"model_class\"],\n        arguments=value[\"model_arguments\"],\n    ),\n),\nValidator(\"document_splitter\", cast=cast_document_splitter),\nValidator(\"retriever_config\", cast=cast_retriever_config),\nValidator(\n    \"retriever\",\n    cast=lambda value: cast_class_path_to_instance(\n        class_path=value[\"retriever_class\"],\n        arguments=value[\"retriever_arguments\"],\n    ),\n),\nValidator(\"clustering_func\", cast=import_attribute_from_module),\nValidator(\n    \"clustering_backend\",\n    cast=lambda value: cast_class_path_to_instance(\n        class_path=value[\"backend_class\"],\n        arguments=value[\"backend_arguments\"],\n    ),\n),\nValidator(\n    \"tree_builder_config\",\n    cast=lambda value: cast_class_path_to_instance(\n        class_path=value[\"config_class\"], arguments=value[\"config_arguments\"]\n    ),\n),\nValidator(\n    \"tree_builder\",\n    cast=lambda value: cast_class_path_to_instance(\n        class_path=value[\"builder_class\"],\n        arguments=value[\"builder_arguments\"],\n    ),\n),\nValidator(\n    \"qa_model\",\n    cast=lambda value: cast_class_path_to_instance(\n        class_path=value[\"model_class\"], arguments=value[\"model_arguments\"]\n    ),\n),\nValidator(\"chunk_size\", default=None),\nValidator(\"chunk_overlap\", default=None),\n]\n</code></pre> <p>Note that the validators are not applied automatically since there is overhead (for example, in casting the embedding models and summarization models).</p> <p>To apply the validators, call <code>settings.validators.validate()</code>.</p> <p>For more details, see the Dynaconf documentation on validators: https://www.dynaconf.com/validation/.</p> <p>The default settings are loaded from the <code>settings.toml</code> and <code>prompts.toml</code>.</p> <ul> <li><code>settings.toml</code> contains the main configuration settings.</li> <li><code>prompts.toml</code> contains the prompts for the user interface.</li> </ul> <p>The settings can be accessed as a dictionary or as attributes. For example:</p> <pre><code>from bookacle.conf import settings\n# Validate the settings\nsettings.validators.validate()\n# Access as attribute\nprint(f\"Default embedding model: {settings.EMBEDDING_MODEL}\")\n# Access as dictionary\nprint(f\"Default QA model: {settings['qa_model']}\")\n</code></pre> <pre><code>Default embedding model: SentenceTransformerEmbeddingModel(model_name='paraphrase-albert-small-v2', use_gpu=False)\nDefault QA model: OllamaQAModel(model_name='qwen2:0.5b')\n</code></pre> <p>For more details on managing settings, see Configuration.</p>"},{"location":"reference/conf/config/#bookacle.conf.config.import_attribute_from_module","title":"import_attribute_from_module","text":"<pre><code>import_attribute_from_module(dotted_path: str) -&gt; Type[Any]\n</code></pre> <p>Imports an attribute (class or function) from a dotted module path.</p> <p>Parameters:</p> <ul> <li> <code>dotted_path</code>               (<code>str</code>)           \u2013            <p>The full path to the attribute in the form of \u2018module.submodule.ClassName\u2019.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Type[Any]</code>           \u2013            <p>The imported attribute (e.g., a class or function).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the module cannot be imported.</p> </li> <li> <code>AttributeError</code>             \u2013            <p>If the attribute does not exist in the module.</p> </li> </ul>"},{"location":"reference/conf/config/#bookacle.conf.config.cast_class_path_to_instance","title":"cast_class_path_to_instance","text":"<pre><code>cast_class_path_to_instance(\n    class_path: str, arguments: dict[str, Any]\n) -&gt; object\n</code></pre> <p>Instantiates a class by its dotted path and passes the provided arguments.</p> <p>Parameters:</p> <ul> <li> <code>class_path</code>               (<code>str</code>)           \u2013            <p>The full path to the class in the form \u2018module.submodule.ClassName\u2019.</p> </li> <li> <code>arguments</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary of arguments to pass to the class constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>object</code>           \u2013            <p>An instance of the class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the module cannot be imported.</p> </li> <li> <code>AttributeError</code>             \u2013            <p>If the class does not exist in the module.</p> </li> </ul>"},{"location":"reference/conf/config/#bookacle.conf.config.cast_document_splitter","title":"cast_document_splitter","text":"<pre><code>cast_document_splitter(\n    value: dict[str, Any]\n) -&gt; DocumentSplitterLike\n</code></pre> <p>Casts a dictionary containing document splitter configuration into an instance of DocumentSplitterLike.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Dictionary containing the class path of the document splitter and its arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DocumentSplitterLike</code>           \u2013            <p>An instance of the document splitter class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If the arguments has the <code>tokenizer_from</code> key              and it doesn\u2019t resolve to an implementation of              EmbeddingModelLike or              SummarizationModelLike.</p> </li> </ul>"},{"location":"reference/conf/config/#bookacle.conf.config.cast_retriever_config","title":"cast_retriever_config","text":"<pre><code>cast_retriever_config(\n    value: dict[str, Any]\n) -&gt; RetrieverLike\n</code></pre> <p>Casts a dictionary containing retriever configuration into an instance of RetrieverLike.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Dictionary with the class path of the retriever and its arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RetrieverLike</code>           \u2013            <p>An instance of the retriever class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If the arguments has the <code>selection_mode</code> key              and it is not a member of the SelectionMode              Enum.</p> </li> </ul>"},{"location":"reference/models/","title":"bookacle.models","text":""},{"location":"reference/models/#bookacle.models","title":"models","text":""},{"location":"reference/models/embedding/","title":"bookacle.models.embedding","text":""},{"location":"reference/models/embedding/#bookacle.models.embedding","title":"embedding","text":"<p>This module defines protocols and concrete implementations for embedding models used for text representation.</p>"},{"location":"reference/models/embedding/#bookacle.models.embedding.EmbeddingModelLike","title":"EmbeddingModelLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the methods and attributes that an embedding model should implement.</p>"},{"location":"reference/models/embedding/#bookacle.models.embedding.EmbeddingModelLike.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer: TokenizerLike\n</code></pre> <p>Returns:</p> <ul> <li> <code>TokenizerLike</code>           \u2013            <p>The tokenizer used by the model.</p> </li> </ul>"},{"location":"reference/models/embedding/#bookacle.models.embedding.EmbeddingModelLike.model_max_length","title":"model_max_length  <code>property</code>","text":"<pre><code>model_max_length: int\n</code></pre> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The maximum length of the input that the model can accept.</p> </li> </ul>"},{"location":"reference/models/embedding/#bookacle.models.embedding.EmbeddingModelLike.embed","title":"embed","text":"<pre><code>embed(\n    text: str | list[str],\n) -&gt; list[float] | list[list[float]]\n</code></pre> <p>Embed the input text or list of texts.</p> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str | list[str]</code>)           \u2013            <p>The input text or list of input texts to embed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float] | list[list[float]]</code>           \u2013            <p>The embeddings of the input text or list of texts.</p> </li> </ul>"},{"location":"reference/models/embedding/#bookacle.models.embedding.SentenceTransformerEmbeddingModel","title":"SentenceTransformerEmbeddingModel","text":"<pre><code>SentenceTransformerEmbeddingModel(\n    model_name: str, *, use_gpu: bool = False\n)\n</code></pre> <p>An embedding model that uses the SentenceTransformer library.</p> <p>It implements the EmbeddingModelLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>use_gpu</code>               (<code>bool</code>)           \u2013            <p>Whether to use the GPU for inference.</p> </li> <li> <code>model</code>               (<code>SentenceTransformer</code>)           \u2013            <p>The SentenceTransformer model.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>use_gpu</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use the GPU for inference.</p> </li> </ul>"},{"location":"reference/models/embedding/#bookacle.models.embedding.SentenceTransformerEmbeddingModel.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer: PreTrainedTokenizerBase\n</code></pre> <p>Returns:</p> <ul> <li> <code>PreTrainedTokenizerBase</code>           \u2013            <p>The tokenizer used by the underlying model.</p> </li> </ul>"},{"location":"reference/models/message/","title":"bookacle.models.message","text":""},{"location":"reference/models/message/#bookacle.models.message","title":"message","text":"<p>This module defines data structures for representing messages exchanged in a conversation with a language model (LLM).</p>"},{"location":"reference/models/message/#bookacle.models.message.Message","title":"Message","text":"<p>               Bases: <code>TypedDict</code></p> <p>A TypedDict that represents a message in a conversation with an LLM.</p> <p>Attributes:</p> <ul> <li> <code>role</code>               (<code>Literal['user', 'assistant', 'system', 'tool']</code>)           \u2013            <p>The role of the message sender.</p> </li> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the message.</p> </li> </ul>"},{"location":"reference/models/qa/","title":"bookacle.models.qa","text":""},{"location":"reference/models/qa/#bookacle.models.qa","title":"qa","text":"<p>This module defines a protocol and an implementation for a Question-Answering (QA) model that processes questions and returns answers with or without streaming capabilities.</p>"},{"location":"reference/models/qa/#bookacle.models.qa.QAModelLike","title":"QAModelLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the methods that a QA model should implement.</p>"},{"location":"reference/models/qa/#bookacle.models.qa.QAModelLike.answer","title":"answer","text":"<pre><code>answer(\n    question: str,\n    context: str,\n    history: list[Message] | None = None,\n    *args,\n    stream: bool = False,\n    **kwargs\n) -&gt; Message | Iterator[Message]\n</code></pre> <p>Answer a question given a context and chat history with or without streaming.</p> <p>Parameters:</p> <ul> <li> <code>question</code>               (<code>str</code>)           \u2013            <p>The question to answer.</p> </li> <li> <code>context</code>               (<code>str</code>)           \u2013            <p>The context for the question.</p> </li> <li> <code>history</code>               (<code>list[Message] | None</code>, default:                   <code>None</code> )           \u2013            <p>The chat history.</p> </li> <li> <code>stream</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to stream the AI response.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Message | Iterator[Message]</code>           \u2013            <p>A single message from the QA model or a stream of messages.</p> </li> </ul>"},{"location":"reference/models/qa/#bookacle.models.qa.OllamaQAModel","title":"OllamaQAModel","text":"<pre><code>OllamaQAModel(model_name: str)\n</code></pre> <p>A QA model that uses the Ollama library.</p> <p>It implements the QAModelLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> </ul>"},{"location":"reference/models/qa/#bookacle.models.qa.OllamaQAModel.answer","title":"answer","text":"<pre><code>answer(\n    question: str,\n    context: str,\n    history: list[Message] | None = None,\n    *args,\n    stream: bool = True,\n    **kwargs\n) -&gt; Message | Iterable[Message]\n</code></pre> <p>Answer a question given a context and chat history with or without streaming.</p> <p>The question and the context are combined into a single message for the QA model, using the following template:</p> <pre><code>CONTEXT:\n&lt;context&gt;\n\nQUERY: &lt;question&gt;\n</code></pre> <p>After combining the question and context, the message is appended to the history (if any) and sent to the QA model.</p> <p>A system prompt can be provided by adding it as the first message in the history.</p>"},{"location":"reference/models/summarization/","title":"bookacle.models.summarization","text":""},{"location":"reference/models/summarization/#bookacle.models.summarization","title":"summarization","text":"<p>This module defines protocols and concrete implementations for summarization models used for summarizing texts in intermediate RAPTOR tree layers.</p>"},{"location":"reference/models/summarization/#bookacle.models.summarization.SummarizationModelLike","title":"SummarizationModelLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the methods and attributes that a summarization model should implement.</p>"},{"location":"reference/models/summarization/#bookacle.models.summarization.SummarizationModelLike.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer: TokenizerLike\n</code></pre> <p>Returns:</p> <ul> <li> <code>TokenizerLike</code>           \u2013            <p>The tokenizer used by the model.</p> </li> </ul>"},{"location":"reference/models/summarization/#bookacle.models.summarization.SummarizationModelLike.summarize","title":"summarize","text":"<pre><code>summarize(text: str | list[str]) -&gt; str | list[str]\n</code></pre> <p>Summarize the input text or list of texts.</p> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str | list[str]</code>)           \u2013            <p>The input text or list of input texts to summarize.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str | list[str]</code>           \u2013            <p>The summary of the input text or list of texts.</p> </li> </ul>"},{"location":"reference/models/summarization/#bookacle.models.summarization.HuggingFaceSummarizationModel","title":"HuggingFaceSummarizationModel","text":"<pre><code>HuggingFaceSummarizationModel(\n    model_name: str,\n    summarization_length: int = 100,\n    *,\n    use_gpu: bool = False\n)\n</code></pre> <p>A class that uses a Hugging Face model for summarization.</p> <p>It implements the SummarizationModelLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the Hugging Face model to use.</p> </li> <li> <code>summarization_length</code>               (<code>int</code>)           \u2013            <p>The maximum length of the summary.</p> </li> <li> <code>use_gpu</code>               (<code>bool</code>)           \u2013            <p>Whether to use the GPU for inference.</p> </li> <li> <code>model</code>               (<code>AutoModelForSeq2SeqLM</code>)           \u2013            <p>The Hugging Face model for summarization.</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The Hugging Face pipeline for summarization.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the Hugging Face model to use.</p> </li> <li> <code>summarization_length</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The maximum length of the summary.</p> </li> <li> <code>use_gpu</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use the GPU for inference.</p> </li> </ul>"},{"location":"reference/models/summarization/#bookacle.models.summarization.HuggingFaceSummarizationModel.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer: PreTrainedTokenizerBase\n</code></pre> <p>Returns:</p> <ul> <li> <code>PreTrainedTokenizerBase</code>           \u2013            <p>The Hugging Face tokenizer used by the underlying model.</p> </li> </ul>"},{"location":"reference/models/summarization/#bookacle.models.summarization.HuggingFaceLLMSummarizationModel","title":"HuggingFaceLLMSummarizationModel","text":"<pre><code>HuggingFaceLLMSummarizationModel(\n    model_name: str,\n    summarization_length: int = 100,\n    *,\n    system_prompt: str = \"\",\n    use_gpu: bool = False\n)\n</code></pre> <p>A class that uses a Hugging Face LLM for summarization.</p> <p>It implements the SummarizationModelLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the Hugging Face LLM to use.</p> </li> <li> <code>summarization_length</code>               (<code>int</code>)           \u2013            <p>The maximum length of the summary.</p> </li> <li> <code>system_prompt</code>               (<code>str</code>)           \u2013            <p>The system prompt passed to the LLM for summarization.</p> </li> <li> <code>use_gpu</code>               (<code>bool</code>)           \u2013            <p>Whether to use the GPU for inference.</p> </li> <li> <code>model</code>               (<code>AutoModelForCausalLM</code>)           \u2013            <p>The Hugging Face LLM for summarization.</p> </li> <li> <code>pipeline</code>               (<code>Pipeline</code>)           \u2013            <p>The Hugging Face pipeline for summarization.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the Hugging Face model to use.</p> </li> <li> <code>summarization_length</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The maximum length of the summary.</p> </li> <li> <code>system_prompt</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The system prompt to pass to the LLM for summarization.</p> </li> <li> <code>use_gpu</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use the GPU for inference.</p> </li> </ul>"},{"location":"reference/models/summarization/#bookacle.models.summarization.HuggingFaceLLMSummarizationModel.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer: PreTrainedTokenizerBase\n</code></pre> <p>Returns:</p> <ul> <li> <code>PreTrainedTokenizerBase</code>           \u2013            <p>The Hugging Face tokenizer used by the underlying model.</p> </li> </ul>"},{"location":"reference/models/summarization/#bookacle.models.summarization.HuggingFaceLLMSummarizationModel.format_as_chat_message","title":"format_as_chat_message","text":"<pre><code>format_as_chat_message(\n    text: str | list[str],\n) -&gt; list[Message] | list[list[Message]]\n</code></pre> <p>Format the input text or list of texts as chat messages.</p> <p>A chat message is a dictionary with the keys \u2018role\u2019 and \u2018content\u2019.</p> If the input is a list of texts <ul> <li>If the system prompt is provided, a list of lists containing the system prompt and user message is returned.</li> <li>If the system prompt is not provided, a list of lists containing the user messages is returned.</li> </ul> If the input is a single text <ul> <li>If the system prompt is provided, a list containing the system prompt and user message is returned.</li> <li>If the system prompt is not provided, a list containing only the user message is returned.</li> </ul> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str | list[str]</code>)           \u2013            <p>The input text or list of texts to format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Message] | list[list[Message]]</code>           \u2013            <p>The formatted chat messages.</p> </li> </ul> <p>Examples:</p> Single Text<pre><code>from bookacle.models.summarization import HuggingFaceLLMSummarizationModel\nmodel = HuggingFaceLLMSummarizationModel(model_name=\"Qwen/Qwen2-0.5B-Instruct\")\ntext = \"This is a test\"\nprint(model.format_as_chat_message(text))\n</code></pre> <pre><code>[{'role': 'user', 'content': 'Summarize the following in not more than 100 words:\\nThis is a test'}]\n</code></pre> Mutliple Texts<pre><code>from bookacle.models.summarization import HuggingFaceLLMSummarizationModel\nmodel = HuggingFaceLLMSummarizationModel(model_name=\"Qwen/Qwen2-0.5B-Instruct\")\ntext = [\"This is a test\", \"This is another test\"]\nprint(model.format_as_chat_message(text))\n</code></pre> <pre><code>[[{'role': 'user', 'content': 'Summarize the following in not more than 100 words:\\nThis is a test'}], [{'role': 'user', 'content': 'Summarize the following in not more than 100 words:\\nThis is another test'}]]\n</code></pre>"},{"location":"reference/models/summarization/#bookacle.models.summarization.HuggingFaceLLMSummarizationModel.summarize","title":"summarize","text":"<pre><code>summarize(text: str | list[str]) -&gt; str | list[str]\n</code></pre> <p>Summarize the input text or list of texts.</p> <p>The input is first formatted into chat messages using format_as_chat_message() and then passed to the underlying LLM for summarization.</p> <p>Each input text is passed to the LLM with the following format:</p> <pre><code>\"Summarize the following in not more than {summarization_length} words:\\n{text}\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str | list[str]</code>)           \u2013            <p>The input text or list of texts to summarize.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str | list[str]</code>           \u2013            <p>The summary of the input text or list of texts.</p> </li> </ul>"},{"location":"reference/tree/","title":"bookacle.tree","text":""},{"location":"reference/tree/#bookacle.tree","title":"tree","text":""},{"location":"reference/tree/builder/","title":"bookacle.tree.builder","text":""},{"location":"reference/tree/builder/#bookacle.tree.builder","title":"builder","text":""},{"location":"reference/tree/builder/#bookacle.tree.builder.TreeBuilderLike","title":"TreeBuilderLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the interface for a RAPTOR tree builder.</p>"},{"location":"reference/tree/builder/#bookacle.tree.builder.TreeBuilderLike.build_from_documents","title":"build_from_documents","text":"<pre><code>build_from_documents(\n    documents: list[Document],\n    chunk_size: int | None = None,\n    chunk_overlap: int | None = None,\n    *args,\n    **kwargs\n) -&gt; Tree\n</code></pre> <p>Build a tree from a list of documents.</p> <p>Parameters:</p> <ul> <li> <code>documents</code>               (<code>list[Document]</code>)           \u2013            <p>A list of documents to build the tree from.</p> </li> <li> <code>chunk_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the chunks to split the documents into.</p> </li> <li> <code>chunk_overlap</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The overlap between the chunks.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional positional arguments.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tree</code>           \u2013            <p>A tree built from the documents.</p> </li> </ul>"},{"location":"reference/tree/builder/#bookacle.tree.builder.ClusterTreeBuilder","title":"ClusterTreeBuilder","text":"<pre><code>ClusterTreeBuilder(config: ClusterTreeConfig)\n</code></pre> <p>A RAPTOR tree builder that clusters nodes at each subsequent tree layer to build the tree.</p> <p>It implements the TreeBuilderLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>config</code>               (<code>RaptorTreeConfig</code>)           \u2013            <p>The configuration for the tree builder.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>ClusterTreeConfig</code>)           \u2013            <p>The configuration for the tree builder.</p> </li> </ul>"},{"location":"reference/tree/builder/#bookacle.tree.builder.ClusterTreeBuilder.create_leaf_nodes","title":"create_leaf_nodes","text":"<pre><code>create_leaf_nodes(\n    chunks: list[Document], embeddings: list[list[float]]\n) -&gt; dict[int, Node]\n</code></pre> <p>Create leaf nodes from the given chunks.</p> <p>Parameters:</p> <ul> <li> <code>chunks</code>               (<code>list[Document]</code>)           \u2013            <p>The chunks to create the leaf nodes from.</p> </li> <li> <code>embeddings</code>               (<code>list[list[float]]</code>)           \u2013            <p>The embeddings of the chunks.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[int, Node]</code>           \u2013            <p>A mapping of the global index to the created leaf nodes.</p> </li> </ul>"},{"location":"reference/tree/builder/#bookacle.tree.builder.ClusterTreeBuilder.create_next_tree_level","title":"create_next_tree_level","text":"<pre><code>create_next_tree_level(\n    clusters: list[list[Node]],\n    first_node_index: int,\n    layer: int,\n) -&gt; dict[int, Node]\n</code></pre> <p>Create the next tree level from the given clusters.</p> For each cluster <ul> <li>The texts of the nodes in the cluster are concatenated.</li> <li>The concatenated text is summarized.</li> <li>The summarized text is embedded.</li> <li>A Node is created with the summarized text, embeddings, and the indices of the children nodes.</li> </ul> <p>Parameters:</p> <ul> <li> <code>clusters</code>               (<code>list[list[Node]]</code>)           \u2013            <p>The clusters to create the next tree level from.</p> </li> <li> <code>first_node_index</code>               (<code>int</code>)           \u2013            <p>The global index of the first node in the new layer.</p> </li> <li> <code>layer</code>               (<code>int</code>)           \u2013            <p>The layer of the tree the clusters belong to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[int, Node]</code>           \u2013            <p>A mapping of the global indices to the created nodes.</p> </li> </ul>"},{"location":"reference/tree/builder/#bookacle.tree.builder.ClusterTreeBuilder.construct_tree","title":"construct_tree","text":"<pre><code>construct_tree(\n    chunks: list[Document],\n    embeddings: list[list[float]],\n    reduction_dimension: int = 10,\n) -&gt; Tree\n</code></pre> <p>Construct a RAPTOR tree from the given chunks and embeddings.</p> <p>The tree is built in a bottom-up manner, starting from the leaf nodes and going up to the root nodes.</p> To build the tree <ul> <li>The leaf nodes are created from the chunks and embeddings.</li> <li>The leaf nodes are clustered to create the next tree level using create_next_tree_level().</li> <li>The process is repeated until the maximum number of layers is reached or the number of nodes in the next level is less than the reduction dimension.</li> </ul> <p>Parameters:</p> <ul> <li> <code>chunks</code>               (<code>list[Document]</code>)           \u2013            <p>The chunks to construct the tree from.</p> </li> <li> <code>embeddings</code>               (<code>list[list[float]]</code>)           \u2013            <p>The embeddings of the chunks.</p> </li> <li> <code>reduction_dimension</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The dimension to reduce the embeddings to before clustering.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tree</code>           \u2013            <p>A RAPTOR tree constructed from the chunks and embeddings.</p> </li> </ul>"},{"location":"reference/tree/builder/#bookacle.tree.builder.ClusterTreeBuilder.build_from_documents","title":"build_from_documents","text":"<pre><code>build_from_documents(\n    documents: list[Document],\n    chunk_size: int | None = None,\n    chunk_overlap: int | None = None,\n) -&gt; Tree\n</code></pre> <p>Build a RAPTOR tree from the given documents.</p> <p>Each document is split into chunks and each chunk is embedded. These are then passed to the construct_tree() method to build the tree.</p> <p>Parameters:</p> <ul> <li> <code>documents</code>               (<code>list[Document]</code>)           \u2013            <p>The documents to build the tree from.</p> </li> <li> <code>chunk_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the chunks to split the documents into.         When <code>None</code>, it defaults to the maximum length supported by the embedding model.</p> </li> <li> <code>chunk_overlap</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The overlap between the chunks. When <code>None</code>, it defaults to half the chunk size.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tree</code>           \u2013            <p>A RAPTOR tree built from the documents.</p> </li> </ul>"},{"location":"reference/tree/clustering/","title":"bookacle.tree.clustering","text":""},{"location":"reference/tree/clustering/#bookacle.tree.clustering","title":"clustering","text":"<p>Clustering module for the tree structure.</p>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.ClusteringBackendLike","title":"ClusteringBackendLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the interface a clustering backend should implement.</p>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.ClusteringBackendLike.cluster","title":"cluster","text":"<pre><code>cluster(\n    embeddings: NDArray[float64], *args, **kwargs\n) -&gt; tuple[dict[int, list[int]], dict[int, list[int]]]\n</code></pre> <p>Cluster the embeddings.</p> <p>Parameters:</p> <ul> <li> <code>embeddings</code>               (<code>NDArray[float64]</code>)           \u2013            <p>The embeddings to cluster.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional arguments to pass to the clustering function.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments to pass to the clustering function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[int, list[int]]</code>           \u2013            <p>The mapping of embeddings to clusters</p> </li> <li> <code>dict[int, list[int]]</code>           \u2013            <p>The mapping of clusters to embeddings.</p> </li> </ul>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.ClusteringFunctionLike","title":"ClusteringFunctionLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the interface a clustering function should implement.</p>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.ClusteringFunctionLike.__call__","title":"__call__","text":"<pre><code>__call__(\n    nodes: list[Node],\n    tokenizer: TokenizerLike,\n    clustering_backend: ClusteringBackendLike | None = None,\n    max_length_in_cluster: int = 3500,\n    reduction_dimension: int = 10,\n    *args,\n    **kwargs\n) -&gt; list[list[Node]]\n</code></pre> <p>Cluster nodes using the given clustering backend.</p> <p>Parameters:</p> <ul> <li> <code>nodes</code>               (<code>list[Node]</code>)           \u2013            <p>The nodes to cluster.</p> </li> <li> <code>tokenizer</code>               (<code>TokenizerLike</code>)           \u2013            <p>The tokenizer to use to calculate the total length of text in each cluster.</p> </li> <li> <code>clustering_backend</code>               (<code>ClusteringBackendLike | None</code>, default:                   <code>None</code> )           \u2013            <p>The clustering backend to use.</p> </li> <li> <code>max_length_in_cluster</code>               (<code>int</code>, default:                   <code>3500</code> )           \u2013            <p>The maximum length of text in a cluster.</p> </li> <li> <code>reduction_dimension</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The dimension to reduce the embeddings to before clustering.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional arguments to pass to the clustering function.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments to pass to the clustering function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Node]]</code>           \u2013            <p>The clustered nodes.</p> </li> </ul>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.GMMClusteringBackend","title":"GMMClusteringBackend","text":"<pre><code>GMMClusteringBackend(\n    reduction_dim: int,\n    max_clusters: int = 50,\n    random_state: int = 42,\n    n_neighbors_global: int | None = None,\n    n_neighbors_local: int = 10,\n    n_clusters_global: int | None = None,\n    n_clusters_local: int | None = None,\n    umap_metric: str = \"cosine\",\n    umap_low_memory: bool = True,\n)\n</code></pre> <p>A Gaussian Mixture Model (GMM) clustering backend.</p> <p>It implements the ClusteringBackendLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>reduction_dim</code>               (<code>int</code>)           \u2013            <p>The dimension to reduce the embeddings to before clustering.</p> </li> <li> <code>max_clusters</code>               (<code>int</code>)           \u2013            <p>The maximum number of clusters to use.</p> </li> <li> <code>random_state</code>               (<code>int</code>)           \u2013            <p>Random state for reproducibility.</p> </li> <li> <code>n_neighbors_global</code>               (<code>int | None</code>)           \u2013            <p>The number of neighbors to use for global clustering.</p> </li> <li> <code>n_neighbors_local</code>               (<code>int | None</code>)           \u2013            <p>The number of neighbors to use for local clustering.</p> </li> <li> <code>n_clusters_global</code>               (<code>int | None</code>)           \u2013            <p>The number of clusters to use for global clustering.</p> </li> <li> <code>n_clusters_local</code>               (<code>int | None</code>)           \u2013            <p>The number of clusters to use for local clustering.</p> </li> <li> <code>umap_metric</code>               (<code>str</code>)           \u2013            <p>The metric to use for UMAP.</p> </li> <li> <code>umap_low_memory</code>               (<code>bool</code>)           \u2013            <p>Whether to use low memory mode for UMAP.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>reduction_dim</code>               (<code>int</code>)           \u2013            <p>The dimension to reduce the embeddings to before clustering.</p> </li> <li> <code>max_clusters</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>The maximum number of clusters to use.</p> </li> <li> <code>random_state</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random state for reproducibility.</p> </li> <li> <code>n_neighbors_global</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of neighbors to use for global clustering.                 When <code>None</code>, it is calculated using Bayesian Information Criterion (BIC).</p> </li> <li> <code>n_neighbors_local</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of neighbors to use for local clustering.                 When <code>None</code>, it is calculated using Bayesian Information Criterion (BIC).</p> </li> <li> <code>n_clusters_global</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of clusters to use for global clustering.</p> </li> <li> <code>n_clusters_local</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of clusters to use for local clustering.</p> </li> <li> <code>umap_metric</code>               (<code>str</code>, default:                   <code>'cosine'</code> )           \u2013            <p>The metric to use for UMAP.</p> </li> <li> <code>umap_low_memory</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use low memory mode for UMAP.</p> </li> </ul>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.GMMClusteringBackend.get_optimal_clusters_count","title":"get_optimal_clusters_count","text":"<pre><code>get_optimal_clusters_count(\n    embeddings: NDArray[float64],\n) -&gt; int\n</code></pre> <p>Get the optimal number of clusters using the Bayesian Information Criterion (BIC).</p> <p>The method fits multiple Gaussian Mixture Models (GMM) to the embeddings and calculates the BIC for each. The number of clusters with the lowest BIC score is selected as the optimal number.</p> <p>Parameters:</p> <ul> <li> <code>embeddings</code>               (<code>NDArray[float64]</code>)           \u2013            <p>The embeddings to cluster.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The optimal number of clusters.</p> </li> </ul>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.GMMClusteringBackend.get_clusters","title":"get_clusters","text":"<pre><code>get_clusters(\n    embeddings: NDArray[float64], n_clusters: int\n) -&gt; NDArray[int64]\n</code></pre> <p>Fit the GMM model to embeddings and return cluster assignments.</p> <p>Parameters:</p> <ul> <li> <code>embeddings</code>               (<code>NDArray[float64]</code>)           \u2013            <p>The embeddings to cluster.</p> </li> <li> <code>n_clusters</code>               (<code>int</code>)           \u2013            <p>The number of clusters to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[int64]</code>           \u2013            <p>The cluster assignments.</p> </li> </ul>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.GMMClusteringBackend.reduce_and_cluster_embeddings","title":"reduce_and_cluster_embeddings","text":"<pre><code>reduce_and_cluster_embeddings(\n    embeddings: NDArray[float64],\n    n_components: int,\n    n_neighbors: int,\n    n_clusters: int | None = None,\n) -&gt; NDArray[int64]\n</code></pre> <p>Reduce the dimensionality of the embeddings using UMAP and cluster them using GMM.</p> <p>It uses umap_reduce_embeddings() to reduce the dimensionality of the embeddings and get_clusters() to cluster them.</p> <p>Generally speaking, this method should be used instead of calling get_clusters() directly.</p> <p>Parameters:</p> <ul> <li> <code>embeddings</code>               (<code>NDArray[float64]</code>)           \u2013            <p>The embeddings to cluster.</p> </li> <li> <code>n_components</code>               (<code>int</code>)           \u2013            <p>The number of components in the reduced space.</p> </li> <li> <code>n_neighbors</code>               (<code>int</code>)           \u2013            <p>The number of neighbors to use for UMAP.</p> </li> <li> <code>n_clusters</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of clusters to use. When <code>None</code>, the optimal number of clusters is         calculated using         get_optimal_clusters_count().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[int64]</code>           \u2013            <p>The cluster assignments.</p> </li> </ul>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.GMMClusteringBackend.cluster_locally","title":"cluster_locally","text":"<pre><code>cluster_locally(\n    embeddings: NDArray[float64],\n    global_cluster_indices: NDArray[int64],\n    n_clusters: int | None = None,\n    n_neighbors: int = 10,\n) -&gt; list[NDArray[int64]]\n</code></pre> <p>Cluster the embeddings of a global cluster locally. In other words, create new clusters from the embeddings of a global cluster.</p> <p>If the number of embeddings in the global cluster is less than or equal to <code>reduction_dim</code>, the global cluster is returned as is in a singleton list.</p> <p>Parameters:</p> <ul> <li> <code>embeddings</code>               (<code>NDArray[float64]</code>)           \u2013            <p>The overall embeddings.</p> </li> <li> <code>global_cluster_indices</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The indices of the embeddings belonging to the global cluster.</p> </li> <li> <code>n_clusters</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of clusters to output.         When <code>None</code>, the optimal number of clusters is calculated using BIC.</p> </li> <li> <code>n_neighbors</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of neighbors to use for UMAP.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[NDArray[int64]]</code>           \u2013            <p>The local clusters.</p> </li> </ul> <p>Examples:</p> <pre><code>import numpy as np\nfrom bookacle.tree.clustering import GMMClusteringBackend\n\nbackend = GMMClusteringBackend(reduction_dim=10)\nembeddings = np.random.rand(100, 768)\nindices = np.random.choice(100, 30)\nclusters = backend.cluster_locally(\n    embeddings=embeddings,\n    global_cluster_indices=indices,\n    n_clusters=5,\n    n_neighbors=10,\n)\nprint(clusters)\n</code></pre> <pre><code>[array([26, 17, 17, 40]), array([13, 60, 51, 13, 64]), array([31, 36, 75,  8, 61, 61,  3, 75]), array([55, 55, 15, 27, 82, 89]), array([11, 96, 47, 56, 25, 44, 37])]\n</code></pre>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.GMMClusteringBackend.cluster","title":"cluster","text":"<pre><code>cluster(\n    embeddings: NDArray[float64],\n) -&gt; tuple[dict[int, list[int]], dict[int, list[int]]]\n</code></pre> <p>Cluster the embeddings.</p> The clustering is done as follows <ul> <li>Global clustering: The embeddings are reduced and clustered globally.                      That is, the entire set of embeddings is clustered.</li> <li>Local clustering: The embeddings of each global cluster are reduced and clustered.</li> <li>At the end, all local clusters are aggregated into a single result.</li> </ul> <p>Parallel from <code>joblib</code> is used to parallelize the clustering of each global cluster.</p> <p>Parameters:</p> <ul> <li> <code>embeddings</code>               (<code>NDArray[float64]</code>)           \u2013            <p>The embeddings to cluster.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[int, list[int]]</code>           \u2013            <p>A mapping of embeddings to clusters</p> </li> <li> <code>dict[int, list[int]]</code>           \u2013            <p>A mapping of clusters to embeddings.</p> </li> </ul> <p>Examples:</p> <pre><code>import numpy as np\nfrom bookacle.tree.clustering import GMMClusteringBackend\n\nbackend = GMMClusteringBackend(reduction_dim=10, n_clusters_global=3)\nembeddings = np.random.rand(10, 768)\nemb_to_clusters, clusters_to_emb = backend.cluster(embeddings=embeddings)\nprint(clusters_to_emb)\n</code></pre> <pre><code>defaultdict(&lt;class 'list'&gt;, {0: [3, 4, 5, 6], 1: [2, 7], 2: [0, 1, 8, 9]})\n</code></pre>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.umap_reduce_embeddings","title":"umap_reduce_embeddings","text":"<pre><code>umap_reduce_embeddings(\n    embeddings: NDArray[float64],\n    n_components: int,\n    neighbors: int = 10,\n    metric: str = \"cosine\",\n    low_memory: bool = True,\n) -&gt; NDArray[float64]\n</code></pre> <p>Reduce the dimensionality of the embeddings using UMAP.</p> <p>Parameters:</p> <ul> <li> <code>embeddings</code>               (<code>NDArray[float64]</code>)           \u2013            <p>The embeddings to reduce.</p> </li> <li> <code>n_components</code>               (<code>int</code>)           \u2013            <p>The number of components in the reduced space.</p> </li> <li> <code>neighbors</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of neighbors to use for UMAP.</p> </li> <li> <code>metric</code>               (<code>str</code>, default:                   <code>'cosine'</code> )           \u2013            <p>The metric to use for UMAP.</p> </li> <li> <code>low_memory</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use low memory mode for UMAP.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[float64]</code>           \u2013            <p>The reduced embeddings.</p> </li> </ul>"},{"location":"reference/tree/clustering/#bookacle.tree.clustering.raptor_clustering","title":"raptor_clustering","text":"<pre><code>raptor_clustering(\n    nodes: list[Node],\n    tokenizer: TokenizerLike,\n    clustering_backend: ClusteringBackendLike | None = None,\n    max_length_in_cluster: int = 3500,\n    reduction_dimension: int = 10,\n) -&gt; list[list[Node]]\n</code></pre> <p>Cluster nodes using RAPTOR clustering.</p> <p>It implements the ClusteringFunctionLike protocol.</p> To cluster the nodes <ul> <li>The nodes are clustered using the given clustering backend.</li> <li>For each cluster:<ul> <li>If the cluster has only one node or the total length of text in the cluster is less than the maximum length, the cluster is kept as is.</li> <li>Otherwise, the cluster is recursively clustered.</li> </ul> </li> </ul>"},{"location":"reference/tree/config/","title":"bookacle.tree.config","text":""},{"location":"reference/tree/config/#bookacle.tree.config","title":"config","text":""},{"location":"reference/tree/config/#bookacle.tree.config.SelectionMode","title":"SelectionMode","text":"<p>               Bases: <code>Enum</code></p> <p>Selection modes supported by the retriever.</p>"},{"location":"reference/tree/config/#bookacle.tree.config.SelectionMode.TOP_K","title":"TOP_K  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TOP_K = 'top_k'\n</code></pre> <p>Selection using Top K.</p>"},{"location":"reference/tree/config/#bookacle.tree.config.SelectionMode.THRESHOLD","title":"THRESHOLD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>THRESHOLD = 'threshold'\n</code></pre> <p>Selection using a threshold value on the distance.</p>"},{"location":"reference/tree/config/#bookacle.tree.config.ClusterTreeConfig","title":"ClusterTreeConfig  <code>dataclass</code>","text":"<pre><code>ClusterTreeConfig(\n    embedding_model: EmbeddingModelLike,\n    summarization_model: SummarizationModelLike,\n    document_splitter: DocumentSplitterLike,\n    clustering_func: ClusteringFunctionLike = raptor_clustering,\n    clustering_backend: ClusteringBackendLike | None = None,\n    max_length_in_cluster: int = 3500,\n    max_num_layers: int = 5,\n)\n</code></pre> <p>Configuration for ClusterTreeBuilder.</p> <p>Parameters:</p> <ul> <li> <code>embedding_model</code>               (<code>EmbeddingModelLike</code>)           \u2013            <p>The embedding model to use.</p> </li> <li> <code>summarization_model</code>               (<code>SummarizationModelLike</code>)           \u2013            <p>The summarization model to use.</p> </li> <li> <code>document_splitter</code>               (<code>DocumentSplitterLike</code>)           \u2013            <p>The document splitter to use.</p> </li> <li> <code>clustering_func</code>               (<code>ClusteringFunctionLike</code>, default:                   <code>raptor_clustering</code> )           \u2013            <p>The clustering function to use.</p> </li> <li> <code>clustering_backend</code>               (<code>ClusteringBackendLike | None</code>, default:                   <code>None</code> )           \u2013            <p>The clustering backend to use.</p> </li> <li> <code>max_length_in_cluster</code>               (<code>int</code>, default:                   <code>3500</code> )           \u2013            <p>The maximum length of a cluster.</p> </li> <li> <code>max_num_layers</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The maximum number of layers</p> </li> </ul>"},{"location":"reference/tree/config/#bookacle.tree.config.ClusterTreeConfig.embedding_tokenizer","title":"embedding_tokenizer  <code>property</code>","text":"<pre><code>embedding_tokenizer: TokenizerLike\n</code></pre> <p>Returns:</p> <ul> <li> <code>TokenizerLike</code>           \u2013            <p>The tokenizer of the embedding model.</p> </li> </ul>"},{"location":"reference/tree/config/#bookacle.tree.config.ClusterTreeConfig.summarization_tokenizer","title":"summarization_tokenizer  <code>property</code>","text":"<pre><code>summarization_tokenizer: TokenizerLike\n</code></pre> <p>Returns:</p> <ul> <li> <code>TokenizerLike</code>           \u2013            <p>The tokenizer of the summarization model.</p> </li> </ul>"},{"location":"reference/tree/config/#bookacle.tree.config.TreeRetrieverConfig","title":"TreeRetrieverConfig  <code>dataclass</code>","text":"<pre><code>TreeRetrieverConfig(\n    embedding_model: EmbeddingModelLike,\n    threshold: float = 0.5,\n    top_k: int = 5,\n    selection_mode: SelectionMode = SelectionMode.TOP_K,\n    max_tokens: int = 3500,\n)\n</code></pre> <p>Configuration for TreeRetriever.</p> <p>Parameters:</p> <ul> <li> <code>embedding_model</code>               (<code>EmbeddingModelLike</code>)           \u2013            <p>The embedding model to use.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold value for selection when using threshold mode for selection.</p> </li> <li> <code>top_k</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The number of top results to return when using top k mode for selection.</p> </li> <li> <code>selection_mode</code>               (<code>SelectionMode</code>, default:                   <code>TOP_K</code> )           \u2013            <p>The selection mode to use.</p> </li> <li> <code>max_tokens</code>               (<code>int</code>, default:                   <code>3500</code> )           \u2013            <p>The maximum number of tokens to retrieve.</p> </li> </ul>"},{"location":"reference/tree/config/#bookacle.tree.config.TreeRetrieverConfig.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer: TokenizerLike\n</code></pre> <p>Returns:</p> <ul> <li> <code>TokenizerLike</code>           \u2013            <p>The tokenizer of the embedding model.</p> </li> </ul>"},{"location":"reference/tree/retriever/","title":"bookacle.tree.retriever","text":""},{"location":"reference/tree/retriever/#bookacle.tree.retriever","title":"retriever","text":""},{"location":"reference/tree/retriever/#bookacle.tree.retriever.RetrieverLike","title":"RetrieverLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol that defines the interface for a tree retriever.</p>"},{"location":"reference/tree/retriever/#bookacle.tree.retriever.RetrieverLike.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    query: str, tree: Tree, *args, **kwargs\n) -&gt; tuple[list[Node], str]\n</code></pre> <p>Retrieve relevant nodes from a tree given a query.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query to retrieve nodes for.</p> </li> <li> <code>tree</code>               (<code>Tree</code>)           \u2013            <p>The tree to retrieve nodes from.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional positional arguments.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>The retrieved nodes.</p> </li> <li> <code>str</code>           \u2013            <p>The concatenated text of the retrieved nodes.</p> </li> </ul>"},{"location":"reference/tree/retriever/#bookacle.tree.retriever.TreeRetriever","title":"TreeRetriever","text":"<pre><code>TreeRetriever(config: TreeRetrieverConfig)\n</code></pre> <p>A tree retriever that retrieves relevant nodes from a tree given a query.</p> <p>It implements the RetrieverLike protocol.</p> <p>Attributes:</p> <ul> <li> <code>config</code>               (<code>TreeRetrieverConfig</code>)           \u2013            <p>The configuration for the retriever.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>TreeRetrieverConfig</code>)           \u2013            <p>The configuration for the retriever.</p> </li> </ul>"},{"location":"reference/tree/retriever/#bookacle.tree.retriever.TreeRetriever.get_relevant_node_indices","title":"get_relevant_node_indices","text":"<pre><code>get_relevant_node_indices(\n    target_embedding: list[float],\n    candidate_nodes: list[Node],\n) -&gt; NDArray[int64]\n</code></pre> <p>Get the relevant node indices given a target embedding and candidate nodes.</p> Nodes are selected in the following manner <ul> <li>The cosine similarity between the target embedding and the candidate node embeddings is computed.</li> <li>The cosine similarities are sorted in descending order.</li> <li>If the selection mode is   SelectionMode.TOP_K, the top <code>k</code> nodes are selected.</li> <li>If the selection mode is   SelectionMode.THRESHOLD,   the nodes with cosine similarity greater than the threshold are selected.</li> </ul> <p>Parameters:</p> <ul> <li> <code>target_embedding</code>               (<code>list[float]</code>)           \u2013            <p>The target embedding to compare against.</p> </li> <li> <code>candidate_nodes</code>               (<code>list[Node]</code>)           \u2013            <p>The candidate nodes to compare against.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[int64]</code>           \u2013            <p>The relevant node indices.</p> </li> </ul>"},{"location":"reference/tree/retriever/#bookacle.tree.retriever.TreeRetriever.get_nodes_within_context","title":"get_nodes_within_context","text":"<pre><code>get_nodes_within_context(\n    candidate_nodes: list[Node],\n) -&gt; list[Node]\n</code></pre> <p>Filter candidate nodes to those that fit within the maximum token length.</p> <p>Parameters:</p> <ul> <li> <code>candidate_nodes</code>               (<code>list[Node]</code>)           \u2013            <p>The candidate nodes to filter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>The filtered candidate nodes.</p> </li> </ul>"},{"location":"reference/tree/retriever/#bookacle.tree.retriever.TreeRetriever.retrieve_collapse","title":"retrieve_collapse","text":"<pre><code>retrieve_collapse(\n    query: str, tree: Tree\n) -&gt; tuple[list[Node], str]\n</code></pre> <p>Retrieve relevant nodes from a tree given a query using the collapsed tree strategy.</p> <p>For more details about the collapsed tree strategy, refer to the RAPTOR paper: https://arxiv.org/pdf/2401.18059.</p> The retrieved nodes are selected in the following manner <ul> <li>The query is embedded using the embedding model.</li> <li>The candidate nodes are all the nodes in the tree.</li> <li>Relevant nodes are retrieved using   get_relevant_node_indices().</li> <li>The nodes are filtered to fit within the maximum token length using   get_nodes_within_context().</li> </ul> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query to retrieve nodes for.</p> </li> <li> <code>tree</code>               (<code>Tree</code>)           \u2013            <p>The tree to retrieve nodes from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>The retrieved nodes.</p> </li> <li> <code>str</code>           \u2013            <p>The concatenated text of the retrieved nodes.</p> </li> </ul>"},{"location":"reference/tree/retriever/#bookacle.tree.retriever.TreeRetriever.retrieve_no_collapse","title":"retrieve_no_collapse","text":"<pre><code>retrieve_no_collapse(\n    query: str, tree: Tree, start_layer: int, end_layer: int\n) -&gt; tuple[list[Node], str]\n</code></pre> <p>Retrieve relevant nodes from a tree given a query using the tree traversal strategy.</p> <p>For more details about the collapsed tree strategy, refer to the RAPTOR paper: https://arxiv.org/pdf/2401.18059.</p> The retrieved nodes are selected in the following manner <ul> <li>The query is embedded using the embedding model.</li> <li>The candidate nodes are all the nodes in the tree from the start layer to the end layer.</li> <li>For each layer<ul> <li>Relevant nodes are retrieved using   get_relevant_node_indices().</li> <li>The process is repeated on the children of the relevant nodes until the end layer is reached   or no more children are available.</li> </ul> </li> <li>The nodes are filtered to fit within the maximum token length using</li> </ul> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query to retrieve nodes for.</p> </li> <li> <code>tree</code>               (<code>Tree</code>)           \u2013            <p>The tree to retrieve nodes from.</p> </li> <li> <code>start_layer</code>               (<code>int</code>)           \u2013            <p>The layer to start retrieving nodes from.</p> </li> <li> <code>end_layer</code>               (<code>int</code>)           \u2013            <p>The layer to stop retrieving nodes at.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>The retrieved nodes.</p> </li> <li> <code>str</code>           \u2013            <p>The concatenated text of the retrieved nodes.</p> </li> </ul>"},{"location":"reference/tree/retriever/#bookacle.tree.retriever.TreeRetriever.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    query: str,\n    tree: Tree,\n    start_layer: int | None = None,\n    end_layer: int | None = None,\n    collapse_tree: bool = True,\n) -&gt; tuple[list[Node], str]\n</code></pre> <p>Retrieve relevant nodes from a tree given a query.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query to retrieve nodes for.</p> </li> <li> <code>tree</code>               (<code>Tree</code>)           \u2013            <p>The tree to retrieve nodes from.</p> </li> <li> <code>start_layer</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The layer to start retrieving nodes from. When <code>None</code>, the root layer is used.</p> </li> <li> <code>end_layer</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The layer to stop retrieving nodes at. When <code>None</code>, the leaf layer is used.</p> </li> <li> <code>collapse_tree</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the collapsed tree strategy.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>The retrieved nodes.</p> </li> <li> <code>str</code>           \u2013            <p>The concatenated text of the retrieved nodes</p> </li> </ul>"},{"location":"reference/tree/structures/","title":"bookacle.tree.structures","text":""},{"location":"reference/tree/structures/#bookacle.tree.structures","title":"structures","text":""},{"location":"reference/tree/structures/#bookacle.tree.structures.Node","title":"Node  <code>dataclass</code>","text":"<pre><code>Node(\n    text: str,\n    index: int,\n    children: set[int],\n    embeddings: list[float],\n    metadata: dict[str, Any] | None = None,\n    layer: int = 0,\n)\n</code></pre> <p>A node in the RAPTOR tree.</p> <p>Attributes:</p> <ul> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The text of the node.</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The global index of the node in the tree.</p> </li> <li> <code>children</code>               (<code>set[int]</code>)           \u2013            <p>The global indices of the children nodes.</p> </li> <li> <code>embeddings</code>               (<code>list[float]</code>)           \u2013            <p>Embeddings of the node\u2019s text.</p> </li> <li> <code>metadata</code>               (<code>dict[str, Any] | None</code>)           \u2013            <p>Metadata about the node\u2019s text.</p> </li> <li> <code>layer</code>               (<code>int</code>)           \u2013            <p>Tree layer the node belongs to.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Node.num_children","title":"num_children  <code>property</code>","text":"<pre><code>num_children: int\n</code></pre> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Number of children nodes.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Node.from_text","title":"from_text  <code>classmethod</code>","text":"<pre><code>from_text(\n    index: int,\n    text: str,\n    embedding_model: EmbeddingModelLike,\n    children_indices: set[int] | None = None,\n    metadata: dict[str, str] | None = None,\n    layer: int = 0,\n) -&gt; Node\n</code></pre> <p>Create a node from text by embedding it using the given embedding model.</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The global index of the node in the tree.</p> </li> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The text of the node.</p> </li> <li> <code>embedding_model</code>               (<code>EmbeddingModelLike</code>)           \u2013            <p>The embedding model to use for embedding the text.</p> </li> <li> <code>children_indices</code>               (<code>set[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The global indices of the children nodes.</p> </li> <li> <code>metadata</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Metadata about the node\u2019s text.</p> </li> <li> <code>layer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Tree layer the node belongs to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>A node created from the text.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Node.from_children","title":"from_children  <code>classmethod</code>","text":"<pre><code>from_children(\n    children: list[Node],\n    embedding_model: EmbeddingModelLike,\n    summarization_model: SummarizationModelLike,\n    index: int,\n    layer: int = 0,\n) -&gt; Node\n</code></pre> <p>Create a node from a list of children nodes by summarizing their texts.</p> <p>The text of the children nodes is concatenated using concatenate_node_texts() and passed to the summarization model to generate a summary.</p> <p>Parameters:</p> <ul> <li> <code>children</code>               (<code>list[Node]</code>)           \u2013            <p>A list of children nodes.</p> </li> <li> <code>embedding_model</code>               (<code>EmbeddingModelLike</code>)           \u2013            <p>The embedding model to use for embedding the summarized text.</p> </li> <li> <code>summarization_model</code>               (<code>SummarizationModelLike</code>)           \u2013            <p>The summarization model to use for summarizing                  the text of the children nodes.</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The global index of the node in the tree.</p> </li> <li> <code>layer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Tree layer the node belongs to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>A node created from the children nodes.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Tree","title":"Tree  <code>dataclass</code>","text":"<pre><code>Tree(\n    all_nodes: dict[int, Node],\n    root_nodes: dict[int, Node],\n    leaf_nodes: dict[int, Node],\n    num_layers: int,\n    layer_to_nodes: dict[int, list[Node]],\n)\n</code></pre> <p>A RAPTOR tree.</p> <p>Attributes:</p> <ul> <li> <code>all_nodes</code>               (<code>dict[int, Node]</code>)           \u2013            <p>All nodes in the tree, mapped to their global indices.</p> </li> <li> <code>root_nodes</code>               (<code>dict[int, Node]</code>)           \u2013            <p>Root nodes in the tree, mapped to their global indices.</p> </li> <li> <code>leaf_nodes</code>               (<code>dict[int, Node]</code>)           \u2013            <p>Leaf nodes in the tree, mapped to their global indices.</p> </li> <li> <code>num_layers</code>               (<code>int</code>)           \u2013            <p>Number of layers in the tree.</p> </li> <li> <code>layer_to_nodes</code>               (<code>dict[int, list[Node]]</code>)           \u2013            <p>Nodes in a layer, mapped to their layer index.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Tree.num_nodes","title":"num_nodes  <code>property</code>","text":"<pre><code>num_nodes: int\n</code></pre> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of nodes in the tree.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Tree.top_layer","title":"top_layer  <code>property</code>","text":"<pre><code>top_layer: int\n</code></pre> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Index of the root layer of the tree.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Tree.tolist","title":"tolist","text":"<pre><code>tolist() -&gt; list[Node]\n</code></pre> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>List of all nodes in the tree.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Tree.get_node","title":"get_node","text":"<pre><code>get_node(index: int) -&gt; Node\n</code></pre> <p>Fetch a node in the tree by its global index.</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The global index of the node to fetch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>The node with the given global index.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Tree.fetch_layer","title":"fetch_layer","text":"<pre><code>fetch_layer(layer: int) -&gt; list[Node]\n</code></pre> <p>Fetch all nodes in a layer of the tree.</p> <p>Parameters:</p> <ul> <li> <code>layer</code>               (<code>int</code>)           \u2013            <p>The layer index to fetch nodes from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Node]</code>           \u2013            <p>List of nodes in the given layer.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.Tree.fetch_node_layer","title":"fetch_node_layer","text":"<pre><code>fetch_node_layer(node_idx: int) -&gt; int\n</code></pre> <p>Fetch the index of the layer a node belongs to in the tree.</p> <p>Parameters:</p> <ul> <li> <code>node_idx</code>               (<code>int</code>)           \u2013            <p>The global index of the node.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The index of the layer the node belongs to.</p> </li> </ul>"},{"location":"reference/tree/structures/#bookacle.tree.structures.concatenate_node_texts","title":"concatenate_node_texts","text":"<pre><code>concatenate_node_texts(nodes: list[Node]) -&gt; str\n</code></pre> <p>Concatenate the texts of a list of nodes.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#example-rag-application","title":"Example RAG Application","text":"<p>This guide demonstrates how to create a Retrieval-Augmented Generation (RAG) pipeline using <code>bookacle</code>. You only need 16 lines of code to get started!</p>"},{"location":"usage/#full-code","title":"Full Code","text":"<p>The complete example shows how to load a document, create a RAPTOR tree for retrieval, and run a query on the document.</p> Code <pre><code>from bookacle.loaders import pymupdf_loader\nfrom bookacle.models.embedding import SentenceTransformerEmbeddingModel\nfrom bookacle.models.message import Message\nfrom bookacle.models.qa import OllamaQAModel\nfrom bookacle.models.summarization import HuggingFaceLLMSummarizationModel\nfrom bookacle.splitters import HuggingFaceTextSplitter\nfrom bookacle.tree.builder import ClusterTreeBuilder\nfrom bookacle.tree.config import ClusterTreeConfig, TreeRetrieverConfig\nfrom bookacle.tree.retriever import TreeRetriever\n\ndocuments = pymupdf_loader(file_path=\"data/the-godfather.pdf\")\n\nembedding_model = SentenceTransformerEmbeddingModel(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\nsummarization_model = HuggingFaceLLMSummarizationModel(\n    model_name=\"Qwen/Qwen2-0.5B-Instruct\",\n    summarization_length=100,\n)\n\nqa_model = OllamaQAModel(model_name=\"qwen2.5:0.5b-instruct\")\n\ndocument_splitter = HuggingFaceTextSplitter(tokenizer=embedding_model.tokenizer)\n\nconfig = ClusterTreeConfig(\n    embedding_model=embedding_model,\n    summarization_model=summarization_model,\n    document_splitter=document_splitter,\n)\n\ntree_builder = ClusterTreeBuilder(config=config)\n\ntree = tree_builder.build_from_documents(documents=documents)\n\nretriever_config = TreeRetrieverConfig(embedding_model=embedding_model)\nretriever = TreeRetriever(config=retriever_config)\n\nquery = \"Who are the cast members of The Godfather?\"\n\n_, context = retriever.retrieve(query=query, tree=tree)\n\nsystem_prompt = \"\"\"You are a helpful assistant, designed to help users understand documents and answer questions on the documents.\nUse your knowledge and the context passed to you to answer user queries.\nThe context will be text extracted from the document. It will be denoted by CONTEXT: in the prompt.\nThe user's query will be denoted by QUERY: in the prompt.\nDo NOT explicitly state that you are referring to the context.\n\"\"\"\n\nhistory = [Message(role=\"system\", content=system_prompt)]\n\nanswer = qa_model.answer(\n    question=query, context=context, stream=False, history=history\n)\n\nprint(f\"Answer:\\n{answer['content']}\")\n</code></pre>"},{"location":"usage/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<p>We now walk through the code step-by-step with explanations of each part.</p> Imports <pre><code>from bookacle.loaders import pymupdf_loader\nfrom bookacle.models.embedding import SentenceTransformerEmbeddingModel\nfrom bookacle.models.qa import OllamaQAModel\nfrom bookacle.models.summarization import HuggingFaceLLMSummarizationModel\nfrom bookacle.splitters import HuggingFaceTextSplitter\nfrom bookacle.tree.builder import ClusterTreeBuilder\nfrom bookacle.tree.config import ClusterTreeConfig, TreeRetrieverConfig\nfrom bookacle.tree.retriever import TreeRetriever\nfrom bookacle.models.message import Message\n</code></pre> <p>We start by loading the data file using <code>pymupdf_loader()</code>, which uses PyMuPDF to load the PDF file as text. The example uses the first 2 pages (when exported in A3) of the Wikipedia entry on The Godfather:</p> <pre><code>documents = pymupdf_loader(file_path=\"data/the-godfather.pdf\")\n\nprint(f\"Number of documents: {len(documents)}\")\nprint(f\"First document:\\n{documents[0]}\")\n</code></pre> <pre><code>Number of documents: 2\nFirst document:\n{'page_content': 'The Godfather\\nTheatrical release poster\\nDirected by\\nFrancis Ford Coppola\\nScreenplay by\\nMario Puzo\\nFrancis Ford Coppola\\nBased on\\nThe Godfather\\nby Mario Puzo\\nProduced by\\nAlbert S. Ruddy\\nStarring\\nMarlon Brando\\nAl Pacino\\nJames Caan\\nRichard Castellano\\nRobert Duvall\\nSterling Hayden\\nJohn Marley\\nRichard Conte\\nDiane Keaton\\nCinematography\\nGordon Willis\\nEdited by\\nWilliam Reynolds\\nPeter Zinner\\nMusic by\\nNino Rota\\nProduction\\ncompanies\\nParamount Pictures\\nAlfran Productions\\nDistributed by\\nParamount Pictures\\nRelease dates\\nMarch\\xa014,\\xa01972 (Loew\\'s\\nState Theatre)\\nMarch\\xa024,\\xa01972 (United\\nStates)\\nRunning time\\n175 minutes\\nCountry\\nUnited States\\nLanguage\\nEnglish\\nBudget\\n$6\u20137.2 million\\nBox o\ufb03ce\\n$250\u2013291 million\\nThe Godfather\\nThe Godfather is a 1972 American epic gangster film directed by Francis Ford Coppola, who\\nco-wrote the screenplay with Mario Puzo, based on Puzo\\'s best-selling 1969 novel. The film stars\\nan ensemble cast including Marlon Brando, Al Pacino, James Caan, Richard Castellano, Robert\\nDuvall,  Sterling  Hayden,  John  Marley,  Richard  Conte  and  Diane  Keaton.  It  is  the  first\\ninstallment in The Godfather trilogy, chronicling the Corleone family under patriarch Vito\\nCorleone (Brando) from 1945 to 1955. It focuses on the transformation of his youngest son,\\nMichael Corleone (Pacino), from reluctant family outsider to ruthless mafia boss.\\nParamount Pictures obtained the rights to the novel for $80,000, before it gained popularity.\\nStudio executives had trouble finding a director; the first few candidates turned down the\\nposition before Coppola signed on to direct the film but disagreement followed over casting\\nseveral  characters,  in  particular,  Vito  (Brando)  and  Michael  (Pacino).  Filming  took  place\\nprimarily in locations around New York City and Sicily, and it was completed ahead of schedule.\\nThe score was composed principally by Nino Rota, with additional pieces by Carmine Coppola.\\nThe Godfather premiered at the Loew\\'s State Theatre on March 14, 1972, and was widely\\nreleased in the United States on March 24, 1972. It was the highest-grossing film of 1972, and\\nwas for a time the highest-grossing film ever made, earning between $250 and $291 million at\\nthe box office. The film was acclaimed by critics and audiences, who praised its performances\u2014\\nparticularly those of Brando and Pacino\u2014direction, screenplay, story, cinematography, editing,\\nscore and portrayal of the mafia. The Godfather launched the successful careers of Coppola,\\nPacino and other relative newcomers in the cast and crew. At the 45th Academy Awards, the film\\nwon Best Picture, Best Actor (Brando) and Best Adapted Screenplay (for Puzo and Coppola). In\\naddition, the seven other Oscar nominations included Pacino, Caan and Duvall, all for Best\\nSupporting Actor, and Coppola for Best Director.\\nThe Godfather is regarded as one of the greatest and most influential films ever made, as well as\\na landmark of the gangster genre. It was selected for preservation in the U.S. National Film\\nRegistry  of  the  Library  of  Congress  in  1990,  being  deemed  \"culturally,  historically,  or\\naesthetically significant\" and is ranked the second-greatest film in American cinema (behind\\nCitizen Kane) by the American Film Institute. It was followed by sequels The Godfather Part II\\n(1974) and The Godfather Part III (1990). Pauline Kael wrote that \"If ever there was a great\\nexample of how the best popular movies come out of a merger of commerce and art, The\\nGodfather is it.\"\\nIn 1945, the New York City Corleone family don, Vito Corleone, listens to requests during his\\ndaughter Connie\\'s wedding to Carlo Rizzi. Vito\\'s youngest son Michael, a Marine who has thus\\nfar stayed out of the family business, introduces his girlfriend, Kay Adams, to his family at the\\nreception. Johnny Fontane, a popular singer and Vito\\'s godson, seeks Vito\\'s help in securing a\\nmovie role. Vito sends his consigliere, Tom Hagen, to persuade studio president Jack Woltz to\\noffer Johnny the part. Woltz refuses Hagen\\'s request at first, but soon complies after finding the severed head of his prized stud horse in his\\nbed.\\nAs Christmas approaches, drug baron Virgil \"The Turk\" Sollozzo asks Vito to invest in his narcotics business and for police protection. Vito\\ndeclines, citing that involvement in narcotics would alienate his political connections. Suspicious of Sollozzo\\'s partnership with the Tattaglia\\ncrime family, Vito sends his enforcer Luca Brasi to the Tattaglias on an espionage mission. Brasi is garroted to death during the initial meeting.\\nLater, enforcers gun down Vito and coerce Hagen into a meeting. With Vito\\'s first-born Sonny now in command, Sollozzo pressures Hagen to\\npersuade Sonny to accept the narcotics deal. Vito survives the shooting and is visited in the hospital by Michael, who finds him unprotected\\nafter NYPD officers on Sollozzo\\'s payroll clear out Vito\\'s guards. Michael thwarts the attempt on his father\\'s life but is beaten by corrupt police\\ncaptain Mark McCluskey. After the attempted hit at the hospital, Sonny retaliates with a hit on Bruno Tattaglia. Sollozzo and McCluskey\\nrequest to meet with Michael and settle the dispute. Michael feigns interest and agrees to meet, but hatches a plan with Sonny and Corleone\\ncapo Clemenza to kill them and go into hiding. Michael meets Sollozzo and McCluskey at a Bronx restaurant; after retrieving a handgun\\nplanted in the bathroom by Clemenza, he shoots both men dead.\\nDespite a clampdown by the authorities for the killing of a police captain, the Five Families erupt in open warfare. Michael takes refuge in\\nSicily and Fredo, Vito\\'s second son, is sheltered by Moe Greene in Las Vegas. In Sicily, Michael meets and marries a local woman, Apollonia.\\nSonny publicly attacks and threatens Carlo for physically abusing Connie. When he abuses her again, Sonny speeds to their home but is\\nambushed and murdered by gangsters at a highway toll booth. Apollonia is killed shortly thereafter by a car bomb intended for Michael.\\nDevastated by Sonny\\'s death and tired of war, Vito sets a meeting with the Five Families. He assures them that he will withdraw his opposition\\nto their narcotics business and forgo avenging Sonny\\'s murder. His safety guaranteed, Michael returns home to enter the family business and\\nPlot\\nThe Godfather - Wikipedia\\nhttps://en.wikipedia.org/wiki/The_Godfather\\n1 of 11\\n19/10/24, 14:37\\n', 'metadata': {'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Mozilla Firefox 131.0.2', 'producer': 'cairo 1.18.0 (https://cairographics.org)', 'creationDate': \"D:20241019143725+05'30\", 'modDate': '', 'trapped': '', 'encryption': None, 'page': 0}}\n</code></pre> More on Document Loaders <p>See Document Loaders for more details on loaders and how you can create your own loaders.</p> <p>Then we create the embedding model and the summarization model that will be used to create the RAPTOR tree. We also create the question-answering model that will be used to answer user queries on the PDF document.</p> <ul> <li>For the embedding model, we are using <code>SentenceTransformerEmbeddingModel</code> to load the <code>all-MiniLM-L6-v2</code> model from the <code>sentence-transformers</code> library.</li> <li>For the summarization model, we are using <code>HuggingFaceLLMSummarizationModel</code> to load the <code>Qwen/Qwen2-0.5B-Instruct</code> LLM from HuggingFace.</li> <li>For the question-answering model, we are using <code>OllamaQAModel</code> to load the <code>qwen2.5:0.5b-instruct</code> model from Ollama.</li> </ul> <pre><code>embedding_model = SentenceTransformerEmbeddingModel(\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n    )\n\nsummarization_model = HuggingFaceLLMSummarizationModel(\n    model_name=\"Qwen/Qwen2-0.5B-Instruct\",\n    summarization_length=100,\n)\n\nqa_model = OllamaQAModel(model_name=\"qwen2.5:0.5b-instruct\")\n\nprint(f\"Embedding Model: {embedding_model}\")\nprint(f\"Summarization Model: {summarization_model}\")\nprint(f\"QA Model: {qa_model}\")\n</code></pre> <pre><code>Embedding Model: SentenceTransformerEmbeddingModel(model_name='sentence-transformers/all-MiniLM-L6-v2', use_gpu=False)\nSummarization Model: HuggingFaceLLMSummarizationModel(model_name='Qwen/Qwen2-0.5B-Instruct', summarization_length=100, system_prompt='', use_gpu=False)\nQA Model: OllamaQAModel(model_name='qwen2.5:0.5b-instruct')\n</code></pre> More on Models <p>See Models for more details on models and how you can create your own models.</p> <p>We then create the document splitter that will be used to split documents into chunks. We are using <code>HuggingFaceTextSplitter</code>, which uses a HuggingFace tokenizer as the length function to decide when a piece of text should be split.</p> <pre><code>document_splitter = HuggingFaceTextSplitter(tokenizer=embedding_model.tokenizer)\n\nprint(document_splitter)\n</code></pre> <pre><code>HuggingFaceTextSplitter(tokenizer=BertTokenizerFast(name_or_path='sentence-transformers/all-MiniLM-L6-v2', vocab_size=30522, model_max_length=256, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}, separators=['\\n\\n', '\\n', '.', '!', '?'])\n</code></pre> More on Document Splitters <p>See Document Splitters for more details on splitters and how you can create your own splitters.</p> <p>You can also use custom tokenizers by implementing the <code>TokenizerLike</code> protocol. See Tokenizers for more details.</p> <p>Next, we create the RAPTOR tree using <code>ClusterTreeBuilder</code>, which implements the methodology in the RAPTOR paper:</p> <ul> <li>Split the documents into chunks and create the leaf nodes from these chunks.</li> <li>For each subsequent layer:<ul> <li>Cluster the nodes in the previous layer.</li> <li>For each cluster, concatenate the texts of the nodes, summarize the text, embed the summary and create a node.</li> <li>Create the layer using the nodes of each cluster.</li> </ul> </li> <li>Repeat the process till clustering is no longer possible.</li> </ul> <pre><code>config = ClusterTreeConfig(\n    embedding_model=embedding_model,\n    summarization_model=summarization_model,\n    document_splitter=document_splitter,\n)\n\ntree_builder = ClusterTreeBuilder(config=config)\n\ntree = tree_builder.build_from_documents(documents=documents)\n\nprint(f\"Tree: {tree}\")\n</code></pre> <pre><code>Tree: Tree(num_layers=2)\n</code></pre> More on Building the RAPTOR tree <p>See Building the RAPTOR tree for more details on building the raptor tree and how you can define your own methodology to build the tree.</p> <p>You can also customize the clustering method used by <code>ClusterTreeBuilder</code> by changing the clustering function and the clustering backend. See Clustering Support for more details.</p> <p>We then create the retriever using <code>TreeRetriever</code>, which implements both the collapsed-tree and tree-traversal methods from the RAPTOR paper:</p> <pre><code>retriever_config = TreeRetrieverConfig(embedding_model=embedding_model)\nretriever = TreeRetriever(config=retriever_config)\n\nprint(f\"Retriever: {retriever}\")\n</code></pre> <pre><code>Retriever: TreeRetriever()\n</code></pre> More on Retrievers <p>See Retriever for more details on retrievers and how you can implement your own retrievers.</p> <p>Then, we send a query to the retriever and fetch relevant context:</p> <pre><code>query = \"Who are the cast members of The Godfather?\"\n\n_, context = retriever.retrieve(query=query, tree=tree)\n\nprint(f\"Retrieved context:\\n{context}\")\n</code></pre> <pre><code>Retrieved context:\nTony Giorgio plays Bruno Tattaglia, Vito Scotti plays Nazorine, Tere Livrano plays Theresa Hagen: Tom's wife, Victor Rendina plays Philip Tattaglia, Jeannie Linero plays Lucy Mancini, and Julie Gregg plays Sandra Corleone. The movie is based on Mario Puzo's The Godfather, which remains on The New York Times Best Seller list for six decades. It stars Burt Lancaster and Danny Thomas\n\nthe office and pay reverence to Michael as \"Don Corleone\".\n\u25aa Marlon Brando as Vito Corleone: crime boss and\npatriarch of the Corleone family\n\u25aa Al Pacino as Michael Corleone: Vito's youngest son\n\u25aa James Caan as Sonny Corleone: Vito's eldest son\n\u25aa Richard Castellano as Peter Clemenza: a caporegime in\nthe Corleone crime family, Sonny's godfather\n\u25aa Robert Duvall as Tom Hagen: Corleone consigliere,\nlawyer\n, and uno\ufb03cial adopted member of the\nCorleone family\n\u25aa Sterling Hayden as Captain McCluskey: a corrupt\npolice captain on Sollozzo's payroll\n\u25aa John Marley as Jack Woltz: Hollywood \ufb01lm producer\nwho is intimidated by the Corleones\n\u25aa Richard Conte as Emilio Barzini: a crime boss of a rival\nfamily\n\n\u25aa Gianni Russo as Carlo Rizzi: Connie's abusive husband\n\u25aa John Cazale as Fredo Corleone: Vito's middle son\n\u25aa Rudy Bond as Cuneo: a crime boss of a rival family\n\u25aa Al Martino as Johnny Fontane: a singer and Vito's\ngodson\n\u25aa Morgana King as Carmela Corleone: Vito's wife\n\u25aa Lenny Montana as Luca Brasi: Vito's enforcer\n\u25aa Johnny Martino as Paulie Gatto: a soldier in the\nCorleone crime family\n\u25aa Salvatore Corsitto as Amerigo Bonasera: the\nundertaker who asks for a favor at Connie's wedding\n\u25aa Richard Bright as Al Neri: the soldier in the Corleone\ncrime family who becomes Michael's enforcer\n\u25aa Alex Rocco as Moe Greene: a Jewish mobster and Las\nVegas casino proprietor\n\u25aa Tony Giorgio as Bruno Tattaglia\n\nco-wrote the screenplay with Mario Puzo, based on Puzo's best-selling 1969 novel. The film stars\nan ensemble cast including Marlon Brando, Al Pacino, James Caan, Richard Castellano, Robert\nDuvall,  Sterling  Hayden,  John  Marley,  Richard  Conte  and  Diane  Keaton.  It  is  the  first\ninstallment in The Godfather trilogy, chronicling the Corleone family under patriarch Vito\nCorleone (Brando) from 1945 to 1955. It focuses on the transformation of his youngest son,\nMichael Corleone (Pacino), from reluctant family outsider to ruthless mafia boss.\nParamount Pictures obtained the rights to the novel for $80,000, before it gained popularity.\nStudio executives had trouble finding a director; the first few candidates turned down the\nposition before Coppola signed on to direct the film but disagreement followed over casting\nseveral  characters,  in  particular,  Vito  (Brando)  and  Michael  (Pacino).  Filming  took  place\nprimarily in locations around New York City and Sicily, and it was completed ahead of schedule.\n\n\u25aa Al Lettieri as Virgil Sollozzo: an adversary who\nattempts to pressure Vito to get into the drug\nbusiness, backed by the Tattaglia family\n\u25aa Diane Keaton as Kay Adams-Corleone: Michael's\ngirlfriend and, later\n, second wife\n\u25aa Abe Vigoda as Salvatore Tessio: a caporegime in the\nCorleone crime family\n\u25aa Talia Shire as Connie Corleone: Vito's only daughter\n\u25aa Gianni Russo as Carlo Rizzi: Connie's abusive husband\n\u25aa John Cazale as Fredo Corleone: Vito's middle son\n\u25aa Rudy Bond as Cuneo: a crime boss of a rival family\n\u25aa Al Martino as Johnny Fontane: a singer and Vito's\ngodson\n\u25aa Morgana King as Carmela Corleone: Vito's wife\n\u25aa Lenny Montana as Luca Brasi: Vito's enforcer\n</code></pre> <p>Finally, we set the system prompt (optional) for the question-answering model and get an answer from the it:</p> <pre><code>system_prompt = \"\"\"You are a helpful assistant, designed to help users understand documents and answer questions on the documents.\nUse your knowledge and the context passed to you to answer user queries.\nThe context will be text extracted from the document. It will be denoted by CONTEXT: in the prompt.\nThe user's query will be denoted by QUERY: in the prompt.\nDo NOT explicitly state that you are referring to the context.\n\"\"\"\n\nhistory = [Message(role=\"system\", content=system_prompt)]\n\nanswer = qa_model.answer(question=query, context=context, stream=False, history=history)\n\nprint(f\"Answer:\\n{answer['content']}\")\n</code></pre> <pre><code>Answer:\nThe cast members of The Godfather are Tony Giorgio, Marlon Brando, Al Pacino, James Caan, Richard Castellano, Robert Duvall, John Marley, Richard Conte, Diane Keaton, Al Lettieri, Diane Keaton, Abe Vigoda, Talia Shire, Gianni Russo, John Cazale, Rudy Bond, Lenny Montana, and Morgana King.\n</code></pre>"},{"location":"usage/#chat-interface","title":"Chat Interface","text":"<p><code>bookacle</code> comes with a built-in terminal-based chat interface powered by <code>rich</code> and <code>prompt-toolkit</code>, which supports the following:</p> <ul> <li>Autocompletion in the chat.</li> <li>Custom user avatars.</li> <li>Markdown rendering.</li> <li>Streaming output with a nice progress bar.</li> <li>Pass a system prompt to the question-answering model.</li> <li>Store chat history in a file as you chat, etc.</li> </ul>"},{"location":"usage/#launch-from-a-script","title":"Launch from a script","text":"<p>The chat interface can be launched in a script by using <code>Chat</code>.</p> <pre><code>from rich.console import Console\nfrom bookacle.chat import Chat\n\nconsole = Console()\n\nchat = Chat(\n    retriever=retriever,\n    qa_model=qa_model,\n    console=console,\n)\n\nsystem_prompt = \"\"\"You are a helpful assistant, designed to help users understand documents and answer questions on the documents.\nUse your knowledge and the context passed to you to answer user queries.\nThe context will be text extracted from the document. It will be denoted by CONTEXT: in the prompt.\nThe user's query will be denoted by QUERY: in the prompt.\nAlways respond in Markdown.\n\"\"\"\n\nchat.run(tree=tree, stream=True, system_prompt=system_prompt)\n</code></pre> <p>Here is an example interaction:</p> <p></p>"},{"location":"usage/#terminal-based-chat","title":"Terminal-based Chat","text":"<p>You can also use the chat via the CLI to interact with your documents.</p> <pre><code>$ bookacle --help\n</code></pre> <pre><code> Usage: bookacle [OPTIONS] FILE_PATH                                                                                                                                                                                                                                                                     \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    file_path      FILE  Path to the PDF file. [required]                                                                                                                                                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --loader              -l      [pymupdf4llm|pymupdf]  Loader to use. [default: pymupdf4llm]                                                                                                                                                                                                            \u2502\n\u2502 --start-page          -s      INTEGER                The page (0-based) in the PDF file to start reading from. If not provided, defaults to 0, reading from the beginning.                                                                                                                            \u2502\n\u2502 --end-page            -e      INTEGER                The page (0-based) in the PDF file to stop reading at (not inclusive). If not provided, the document will be read till the end.                                                                                                                  \u2502\n\u2502 --user-avatar         -a      TEXT                   Avatar that should be used for the user during chat. [default: \ud83d\udc64]                                                                                                                                                                               \u2502\n\u2502 --history_file        -h      TEXT                   File where chat history should be stored. [default: /home/malay_agr/.bookacle-chat-history.txt]                                                                                                                                                  \u2502\n\u2502 --config-file         -c      FILE                   Custom configuration file. If not provided, the default settings are used.                                                                                                                                                                       \u2502\n\u2502 --prompt-file         -p      FILE                   Custom prompts file. If not provided, the default prompts are used.                                                                                                                                                                              \u2502\n\u2502 --version             -v                             Print version and exit.                                                                                                                                                                                                                          \u2502\n\u2502 --install-completion                                 Install completion for the current shell.                                                                                                                                                                                                        \u2502\n\u2502 --show-completion                                    Show completion for the current shell, to copy it or customize the installation.                                                                                                                                                                 \u2502\n\u2502 --help                                               Show this message and exit.                                                                                                                                                                                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>See Command-Line Interface for more information on the usage.</p>"},{"location":"usage/models/","title":"Models","text":""},{"location":"usage/models/#supported-models","title":"Supported Models","text":"<p><code>bookacle</code> supports three kinds of models:</p> <ul> <li>Embedding Models - These models are used to embed text into a vector.</li> <li>Summarization Models - These models are used to summarize text.</li> <li>Question Answering Models - These models are used for question-answering on PDF documents.</li> </ul> <p><code>bookacle</code> comes with default implementations for all of the models, making it easy to use. All implementations are local out of the box - you do not need an OpenAI key to use <code>bookacle</code> (just good hardware ).</p> <p>Custom models can be implemented easily by implementing the corresponding protocols:</p> <ul> <li><code>EmbeddingModelLike</code> - Protocol for all embedding models.</li> <li><code>SummarizationModelLike</code> - Protocol for all summarization models.</li> <li><code>QAModelLike</code> - Protocol for all question-answering models.</li> </ul> <p>By implementing these protocols, <code>bookacle</code> can practically support any model available in the market.</p>"},{"location":"usage/models/#quick-start","title":"Quick Start","text":"<p>You can get started by using any of the default implementations.</p>"},{"location":"usage/models/#use-an-embedding-model-from-sentence-transformers","title":"Use an embedding model from <code>sentence-transformers</code>","text":"<p><code>bookacle</code> supports any model from the <code>sentence-transformers</code> library via the <code>SentenceTransformerEmbeddingModel</code> class.</p> <p>You can embed a list of texts:</p> <pre><code>from bookacle.models.embedding import SentenceTransformerEmbeddingModel\n\nembedding_model = SentenceTransformerEmbeddingModel(model_name=\"all-MiniLM-L6-v2\")\ntexts = [\"This is a test\", \"This is another text\"]\nembeddings = embedding_model.embed(texts)\nprint(embeddings)\n</code></pre> <pre><code>[[0.030612455680966377, 0.013831389136612415, -0.020843813195824623, 0.01632791757583618, -0.010231463238596916, -0.04798430949449539, -0.017313335090875626, 0.03728746995329857, 0.0458872951567173, 0.03440503776073456, -0.01995977759361267, -0.04465901851654053, -0.013102819211781025, 0.04284117370843887, -0.055393286049366, -0.05897996947169304, 0.013357817195355892, -0.04093948006629944, -0.046640243381261826, 0.030635887756943703, 0.034367453306913376, 0.060174837708473206, -0.059833962470293045, 0.01768563687801361, 0.006318147759884596, -0.011531705968081951, -0.05604183301329613, 0.02306288108229637, 0.035522907972335815, -0.0007312067900784314, -0.0045328824780881405, 0.057125385850667953, 0.06493885815143585, 0.022896159440279007, 0.039082884788513184, 0.015843145549297333, 0.07268378138542175, 0.047734133899211884, 0.008836441673338413, 0.03844047337770462, 0.0178163331001997, -0.09784718602895737, 0.019852813333272934, 0.026004815474152565, 0.004592681769281626, 0.05503907427191734, -0.04667206481099129, 0.03559368848800659, -0.06167946383357048, 0.0011078554671257734, -0.016140859574079514, -0.02640579268336296, -0.07949255406856537, -0.08490555733442307, 0.02612098678946495, -0.0003536582225933671, 0.03115987405180931, -0.02975994348526001, 0.07323474436998367, 0.05357594043016434, -0.015631122514605522, -0.016084633767604828, -0.02865142747759819, 0.03595297411084175, 0.09541890770196915, 0.027765456587076187, -0.04436489939689636, -0.08868944644927979, -0.012653685174882412, -0.05754895135760307, 0.00029446242842823267, 0.011904983781278133, 0.02711832895874977, 0.0427401065826416, 0.029987173154950142, 0.013047449290752411, -0.03863212466239929, -0.09747397154569626, 0.06669681519269943, 0.04018721356987953, -0.09090323001146317, -0.06058499589562416, -0.01149536482989788, 0.02387799136340618, -0.011324130930006504, 0.06687264144420624, 0.08630163967609406, -0.025462999939918518, -0.0988529697060585, -0.006116452161222696, -0.011836444959044456, 0.055986322462558746, -0.06093982234597206, -0.0124645521864295, 0.016563139855861664, -0.021560925990343094, -0.027123969048261642, -0.0020143548026680946, -0.0597769059240818, 0.1048697829246521, -0.002733848989009857, 0.013900971971452236, -0.011673804372549057, 0.013032770715653896, -0.07930494844913483, -0.07446805387735367, 0.035116150975227356, -0.04340590909123421, 0.06082131341099739, 0.00016749520727898926, -0.0566425584256649, -0.026391491293907166, 0.061022572219371796, 0.05601632222533226, 0.017586346715688705, -0.02037360705435276, -0.11545398086309433, 0.04475245624780655, 0.029722360894083977, 0.033025164157152176, 0.08177473396062851, -0.007757130078971386, -0.0009870959911495447, -0.03362540155649185, 0.013657270930707455, -0.022546403110027313, 0.019304458051919937, -6.218771396084904e-33, -0.01287372037768364, -0.041750065982341766, 0.02567782998085022, 0.07849729806184769, -0.01629389449954033, 0.01920202374458313, -0.0353076346218586, 0.048524148762226105, -0.010347978211939335, -0.02134625054895878, -0.02838769555091858, -0.07118804007768631, -0.0011105583980679512, 0.016433585435152054, 0.08224150538444519, 0.11024076491594315, -0.013450153172016144, 0.10640890896320343, -0.07292719185352325, 0.06126890704035759, -0.055247943848371506, 0.031974971294403076, 0.001206150627695024, -0.10626255720853806, -0.08820132166147232, -0.05346548929810524, -0.012100035324692726, -0.0006844053859822452, -0.005452693905681372, 0.026338350027799606, -0.012515079230070114, 0.03960097208619118, -0.07330325990915298, 0.03817977011203766, -0.026006445288658142, -0.022073565050959587, 0.012753617018461227, -0.01975109800696373, -0.011463118717074394, 0.014134258031845093, -0.060215335339307785, -0.04115573689341545, -0.033304862678050995, 0.05939899757504463, 0.06408748775720596, -0.028469707816839218, -0.025994781404733658, -0.03844074904918671, 0.07594479620456696, 0.0013290714705362916, -0.00634453259408474, -0.0017964687431231141, -0.002401819685474038, -0.012808777391910553, -0.04043186455965042, 0.050200529396533966, 0.03893585875630379, 0.017597705125808716, -0.04384371265769005, 0.09622462093830109, 0.038134876638650894, 0.031135767698287964, -0.0758492648601532, 0.017818808555603027, -0.05012990161776543, 0.007719798944890499, -0.04694923013448715, -0.029028410091996193, 0.020347613841295242, 0.0519399493932724, 0.020630426704883575, -0.03577140346169472, 0.026522178202867508, 0.038080524653196335, -0.051192525774240494, -0.06872225552797318, -0.03543980047106743, 0.08038211613893509, -0.03772076219320297, -0.0019484650110825896, 0.037762049585580826, -0.10211655497550964, 0.04411964863538742, -0.07546437531709671, -0.040720872581005096, -0.01922151818871498, -0.02725924737751484, -0.08222347497940063, 0.003876697737723589, -0.061851996928453445, -0.031574055552482605, 0.024813687428832054, 0.004459897987544537, -0.06730470806360245, 0.07732278108596802, 2.7396774800164803e-33, -0.07120928168296814, 0.09144872426986694, -0.0604870580136776, 0.09686953574419022, 0.08657430112361908, -0.02067793533205986, 0.12994472682476044, 0.007910454645752907, -0.08014624565839767, 0.1802346408367157, -0.0012835395755246282, 0.03514110669493675, 0.042759232223033905, -0.032923728227615356, 0.03707115724682808, 0.015503033995628357, 0.06208204850554466, -0.0637759417295456, -0.00826842151582241, -0.031668417155742645, -0.09215668588876724, 0.12675628066062927, 0.05259893462061882, 0.06083960831165314, -0.06681504845619202, 0.041024234145879745, 0.023306608200073242, -0.0696190670132637, -0.0015632979338988662, -0.018594680353999138, 0.015709752216935158, 0.0037315862718969584, -0.06175531446933746, 0.006468655075877905, 0.031019357964396477, -0.013525241985917091, 0.1241416186094284, -0.03099038079380989, -0.053906723856925964, 0.08038649708032608, 0.014954712241888046, 0.11118161678314209, 0.11494535952806473, 0.10129859298467636, -0.008285170421004295, 0.018935466185212135, 0.018787886947393417, -0.09819530695676804, 0.021266572177410126, 0.055501148104667664, -0.06605244427919388, -0.00862936582416296, 0.026670807972550392, 0.06042525917291641, -0.042968593537807465, 0.016999373212456703, -0.0363522469997406, -0.0009375516674481332, 0.024538744240999222, 0.01933225430548191, -0.08255282789468765, 0.07356218248605728, -0.03603252395987511, 0.04258878529071808, -0.0128817493095994, -0.022433863952755928, -0.07963509112596512, 0.07690770924091339, 0.040927913039922714, 0.010309535078704357, 0.0663880705833435, 0.03659835457801819, -0.13319484889507294, -0.05818798765540123, 0.06462288647890091, -0.09328170120716095, -0.04428894445300102, 0.006638913415372372, 0.02975262701511383, -0.043026696890592575, -0.04989458993077278, -0.07609459012746811, 0.003996340092271566, 0.034756213426589966, -0.07185260951519012, 0.09365355968475342, -0.02969425544142723, 0.032247986644506454, -0.04721519351005554, 0.026805879548192024, -0.0013494578888639808, -3.35526347043924e-05, -0.009545459412038326, -0.049362294375896454, -0.015678929165005684, -1.6930899349176798e-08, -0.009994355030357838, -0.049812640994787216, -0.005801562685519457, 0.011708877980709076, -0.030966216698288918, 0.07448813319206238, 0.04264145717024803, -0.053642142564058304, -0.05357576534152031, 0.005138280801475048, 0.09430532902479172, 0.0592377744615078, -0.06483539938926697, 0.047453366219997406, 0.0664837434887886, -0.08378928154706955, 0.013958822935819626, 0.001708249212242663, -0.010309797711670399, 0.07282484322786331, -0.09265510737895966, 0.029341895133256912, 0.030910231173038483, 0.040054284036159515, -0.01109660230576992, 0.056348562240600586, 0.025945425033569336, 0.08576096594333649, 0.003964757081121206, 0.024685867130756378, 0.055565573275089264, 0.06292921304702759, -0.040880974382162094, -0.05910543352365494, -0.02184230647981167, 0.062095899134874344, 0.03349875286221504, -0.03533301129937172, 0.0267812330275774, 0.01788383349776268, -0.10489799827337265, -0.005059277638792992, 0.0010323041351512074, 0.04416537284851074, -0.05984349548816681, -0.04777807369828224, -0.07838761806488037, -0.02046036347746849, -0.030605660751461983, -0.05824664235115051, -0.03258360177278519, -0.005576286930590868, 0.04223371669650078, -0.020661145448684692, 0.011428107507526875, -0.04831383749842644, -0.036218300461769104, -0.010730714537203312, -0.08764959871768951, 0.036847177892923355, 0.11295994371175766, -0.016820374876260757, 0.09417641907930374, -0.04484279081225395], [-0.013409674167633057, 0.05422273650765419, -0.012761354446411133, -0.003200049512088299, 0.03609046712517738, -0.027999019250273705, 0.07846680283546448, -0.013265153393149376, 0.11536920070648193, -0.04115389660000801, 0.0166016835719347, 0.02461712248623371, -0.027169110253453255, -0.006817244458943605, -0.06958436220884323, 0.015025901608169079, 0.030989298596978188, -0.04469304904341698, -0.06639228016138077, 0.002000785432755947, 0.012641465291380882, 0.14097419381141663, -0.018916962668299675, 0.03220295533537865, 0.02342841774225235, 0.06915004551410675, -0.06172248348593712, 0.12996183335781097, 0.028342965990304947, 0.052997082471847534, -0.04949696362018585, 0.11866146326065063, 0.06798027455806732, 0.02636704407632351, 0.021692758426070213, -0.006293952465057373, -0.02747163362801075, 0.10144687443971634, -0.04329666122794151, 0.017107022926211357, -0.005475620273500681, -0.1162223294377327, -0.01428981963545084, 0.02461080811917782, 0.0023441272787749767, 0.029968956485390663, -0.07655058056116104, -0.040692903101444244, 0.03034655563533306, 0.012212011963129044, -0.03760629519820213, -0.04391976818442345, -0.053823985159397125, -0.024345140904188156, 0.04097140207886696, 0.1133681982755661, 0.04634343460202217, -0.04171735420823097, 0.061605993658304214, -0.03524138405919075, 0.03466030955314636, 0.07317081093788147, 0.01769956573843956, 0.04271915555000305, 0.11453939974308014, -0.009656463749706745, -0.050005584955215454, -0.0064838905818760395, -0.15254244208335876, 0.003397691296413541, 0.07279632985591888, 0.029871443286538124, -0.021627379581332207, 0.02484232746064663, -0.03440726175904274, -0.024347681552171707, -0.04441449046134949, -0.051247738301754, -0.0069200098514556885, 0.012414912693202496, -0.045305293053388596, -0.018079020082950592, 0.014311539009213448, 0.008569728583097458, -0.08081069588661194, 0.034178510308265686, -0.027360523119568825, -0.06653930246829987, -0.00910444837063551, -0.0007475292659364641, 0.03391420096158981, 0.0058415113016963005, 0.09097747504711151, 0.007869628258049488, -0.07194098085165024, -0.0475233756005764, -0.11210166662931442, -0.022146044299006462, -0.06701599806547165, 0.13312245905399323, -0.03824518248438835, -0.006490915548056364, 0.009442590177059174, -0.015479057095944881, -0.04820578917860985, -0.10529038310050964, -0.0122987637296319, -0.00735439732670784, 0.026313068345189095, -0.057879652827978134, -0.03335835784673691, -0.007489229552447796, -0.09510820358991623, -0.01833377219736576, 0.04835982993245125, 0.027976877987384796, 0.06150103732943535, -0.0011439473601058125, 0.04285573959350586, 0.06363803893327713, -0.008691895753145218, -0.05781340226531029, -0.044351059943437576, 0.004791758488863707, -0.020956024527549744, -0.09381714463233948, 0.038750771433115005, -2.732741696004644e-33, -0.0026793337892740965, 0.013229786418378353, -0.025732608512043953, 0.02061227709054947, 0.06149739399552345, 0.03521405905485153, -0.04310207813978195, -0.04096528887748718, -0.04375782608985901, -0.11426932364702225, -0.0614008754491806, -0.04373839125037193, 0.03776666522026062, -0.07069312036037445, -0.01711208000779152, -0.049604203552007675, -0.03836197406053543, 0.1185254454612732, 0.07539859414100647, 0.032252222299575806, -0.03706010431051254, 0.1127210482954979, 0.028489885851740837, -0.05077983811497688, -0.01520354300737381, 0.031775761395692825, 0.03518928214907646, 0.00552782230079174, 0.018520168960094452, 0.025301096960902214, 0.012308867648243904, 0.05100933089852333, 0.03741074353456497, -0.0235601793974638, 0.03964461386203766, 0.019408168271183968, 0.028130052611231804, -0.05097424238920212, -0.01400467287749052, -0.016896693035960197, 0.010420695878565311, -0.009123064577579498, -0.023002108559012413, -0.05087205395102501, 0.07496114820241928, 0.06271716207265854, 0.01702369935810566, -0.022203098982572556, -0.02816150337457657, -0.05796828866004944, -0.031894925981760025, 0.03728947788476944, 0.051321841776371, 0.006152238231152296, 0.015158654190599918, -0.003982344642281532, -0.005508152302354574, 0.0738743245601654, -0.004804818890988827, -0.006980707868933678, 0.003288507228717208, 0.02226649597287178, -0.0400947704911232, 0.014281767420470715, -0.023971589282155037, 0.01445505116134882, -0.016815539449453354, -0.017695611342787743, 0.039227165281772614, -0.026634477078914642, -0.06059538200497627, -0.018703101202845573, 0.02909553423523903, 0.024894120171666145, -0.009444073773920536, -0.07256387919187546, -0.0020334599539637566, 0.033345289528369904, 0.024411972612142563, 0.006007564719766378, -0.06679173558950424, -0.11798728257417679, -0.003403669223189354, 0.006011255085468292, -0.0025281149428337812, 0.03225762024521828, 0.02124553918838501, -0.0964810699224472, -0.0011573724914342165, 0.10348601639270782, -0.1366959810256958, 0.0394381619989872, -0.03272629529237747, 0.0059622968547046185, 0.03281940147280693, 9.2006503035387e-34, -0.048096396028995514, 0.06512829661369324, -0.04944799467921257, -0.021626846864819527, 0.0917009487748146, 0.025447316467761993, 0.05869617313146591, 0.010040703229606152, 0.013440764509141445, 0.06642237305641174, -0.026902498677372932, 0.05462001636624336, 0.09447095543146133, -0.05588965117931366, 0.03945977985858917, 0.03600950911641121, 0.1140575110912323, 0.02209986001253128, -0.003542839316651225, 0.026985537260770798, -0.047703567892313004, 0.04990284889936447, -0.051563311368227005, 0.0596298985183239, 0.030864978209137917, 0.015144355595111847, 0.05397241562604904, 0.024021295830607414, -0.0475536473095417, -0.021129969507455826, 0.011311481706798077, -0.00578182702884078, 0.015052086673676968, -0.027789870277047157, 0.0034026377834379673, -0.018340326845645905, 0.09134574979543686, -0.04772049933671951, -0.14426125586032867, 0.08465246856212616, 0.0858253464102745, 0.014751513488590717, 0.025063764303922653, 0.0035462044179439545, -0.00598553754389286, -0.018565179780125618, -0.06410066038370132, -0.0381188802421093, 0.014023351483047009, 0.05712719261646271, -0.021907728165388107, -0.11641982942819595, -0.05747463181614876, -0.0027848200406879187, -0.0722687691450119, -0.002128751017153263, -0.0041648312471807, 0.01990804448723793, 0.02579299919307232, -0.07708142697811127, -0.010573351755738258, 0.03548162057995796, -0.08939314633607864, 0.07463721930980682, 0.09440936893224716, -0.05008842051029205, -0.03960919380187988, 0.04992206022143364, -0.03649460896849632, 0.0305943563580513, 0.0391831248998642, -0.08915091305971146, -0.16025546193122864, -0.03586980327963829, -0.012061011977493763, -0.023771710693836212, -0.005546705797314644, -0.015973707661032677, -0.09265945851802826, -0.007542621344327927, 0.11325205117464066, -0.045546334236860275, -0.004810459911823273, 0.07179175317287445, -0.014281295239925385, 0.03135373815894127, -0.05736076459288597, 0.014675918035209179, -0.010075701400637627, 0.004987342748790979, -0.07707076519727707, 0.033745113760232925, 0.010859940201044083, 0.07166961580514908, -0.02194942906498909, -1.7299866428288624e-08, 0.022551756352186203, -0.10146799683570862, -0.08867359906435013, -0.04218815639615059, 0.030636783689260483, 0.06206211820244789, 0.01646772399544716, -0.13293850421905518, -0.004454590380191803, -0.02087506651878357, 0.1039414331316948, 0.07758404314517975, 0.0017092915950343013, -0.004377019125968218, 0.024237168952822685, -0.006486197933554649, -0.0028265404980629683, -0.05986762046813965, 0.02342396415770054, -0.015133627690374851, 0.003996767569333315, 0.04529254138469696, 0.002878433559089899, -0.006209454499185085, -0.03688574582338333, 0.06549371778964996, -0.021985584869980812, 0.014119823463261127, -0.030616212636232376, 6.007072897773469e-06, 0.06446658819913864, 0.05123062804341316, -0.04049473628401756, -0.05678323283791542, -0.005555553361773491, 0.010750820860266685, 0.055623818188905716, -0.005718015134334564, 0.07253670692443848, 0.031946249306201935, 0.02091604843735695, -0.01535823568701744, 0.0013207634910941124, 0.05684935301542282, 0.016686629503965378, 0.010076011531054974, 0.03392864018678665, -0.09180062264204025, 0.022942716255784035, -0.02175895683467388, 0.017377551645040512, -0.03734732046723366, 0.06608318537473679, 0.031051473692059517, 0.04631378874182701, -0.008197366259992123, 0.019672859460115433, 0.05536578968167305, -0.07264737784862518, 0.04302215576171875, 0.07741106301546097, -0.032122619450092316, 0.08090478926897049, -0.03179511800408363]]\n</code></pre> <p>You can also embed a single text:</p> <pre><code>from bookacle.models.embedding import SentenceTransformerEmbeddingModel\n\nembedding_model = SentenceTransformerEmbeddingModel(model_name=\"all-MiniLM-L6-v2\")\ntext = \"This is a test\"\nembeddings = embedding_model.embed(text)\nprint(embeddings)\n</code></pre> <pre><code>[0.030612455680966377, 0.013831389136612415, -0.020843813195824623, 0.01632791757583618, -0.010231463238596916, -0.04798430949449539, -0.017313335090875626, 0.03728746995329857, 0.0458872951567173, 0.03440503776073456, -0.01995977759361267, -0.04465901851654053, -0.013102819211781025, 0.04284117370843887, -0.055393286049366, -0.05897996947169304, 0.013357817195355892, -0.04093948006629944, -0.046640243381261826, 0.030635887756943703, 0.034367453306913376, 0.060174837708473206, -0.059833962470293045, 0.01768563687801361, 0.006318147759884596, -0.011531705968081951, -0.05604183301329613, 0.02306288108229637, 0.035522907972335815, -0.0007312067900784314, -0.0045328824780881405, 0.057125385850667953, 0.06493885815143585, 0.022896159440279007, 0.039082884788513184, 0.015843145549297333, 0.07268378138542175, 0.047734133899211884, 0.008836441673338413, 0.03844047337770462, 0.0178163331001997, -0.09784718602895737, 0.019852813333272934, 0.026004815474152565, 0.004592681769281626, 0.05503907427191734, -0.04667206481099129, 0.03559368848800659, -0.06167946383357048, 0.0011078554671257734, -0.016140859574079514, -0.02640579268336296, -0.07949255406856537, -0.08490555733442307, 0.02612098678946495, -0.0003536582225933671, 0.03115987405180931, -0.02975994348526001, 0.07323474436998367, 0.05357594043016434, -0.015631122514605522, -0.016084633767604828, -0.02865142747759819, 0.03595297411084175, 0.09541890770196915, 0.027765456587076187, -0.04436489939689636, -0.08868944644927979, -0.012653685174882412, -0.05754895135760307, 0.00029446242842823267, 0.011904983781278133, 0.02711832895874977, 0.0427401065826416, 0.029987173154950142, 0.013047449290752411, -0.03863212466239929, -0.09747397154569626, 0.06669681519269943, 0.04018721356987953, -0.09090323001146317, -0.06058499589562416, -0.01149536482989788, 0.02387799136340618, -0.011324130930006504, 0.06687264144420624, 0.08630163967609406, -0.025462999939918518, -0.0988529697060585, -0.006116452161222696, -0.011836444959044456, 0.055986322462558746, -0.06093982234597206, -0.0124645521864295, 0.016563139855861664, -0.021560925990343094, -0.027123969048261642, -0.0020143548026680946, -0.0597769059240818, 0.1048697829246521, -0.002733848989009857, 0.013900971971452236, -0.011673804372549057, 0.013032770715653896, -0.07930494844913483, -0.07446805387735367, 0.035116150975227356, -0.04340590909123421, 0.06082131341099739, 0.00016749520727898926, -0.0566425584256649, -0.026391491293907166, 0.061022572219371796, 0.05601632222533226, 0.017586346715688705, -0.02037360705435276, -0.11545398086309433, 0.04475245624780655, 0.029722360894083977, 0.033025164157152176, 0.08177473396062851, -0.007757130078971386, -0.0009870959911495447, -0.03362540155649185, 0.013657270930707455, -0.022546403110027313, 0.019304458051919937, -6.218771396084904e-33, -0.01287372037768364, -0.041750065982341766, 0.02567782998085022, 0.07849729806184769, -0.01629389449954033, 0.01920202374458313, -0.0353076346218586, 0.048524148762226105, -0.010347978211939335, -0.02134625054895878, -0.02838769555091858, -0.07118804007768631, -0.0011105583980679512, 0.016433585435152054, 0.08224150538444519, 0.11024076491594315, -0.013450153172016144, 0.10640890896320343, -0.07292719185352325, 0.06126890704035759, -0.055247943848371506, 0.031974971294403076, 0.001206150627695024, -0.10626255720853806, -0.08820132166147232, -0.05346548929810524, -0.012100035324692726, -0.0006844053859822452, -0.005452693905681372, 0.026338350027799606, -0.012515079230070114, 0.03960097208619118, -0.07330325990915298, 0.03817977011203766, -0.026006445288658142, -0.022073565050959587, 0.012753617018461227, -0.01975109800696373, -0.011463118717074394, 0.014134258031845093, -0.060215335339307785, -0.04115573689341545, -0.033304862678050995, 0.05939899757504463, 0.06408748775720596, -0.028469707816839218, -0.025994781404733658, -0.03844074904918671, 0.07594479620456696, 0.0013290714705362916, -0.00634453259408474, -0.0017964687431231141, -0.002401819685474038, -0.012808777391910553, -0.04043186455965042, 0.050200529396533966, 0.03893585875630379, 0.017597705125808716, -0.04384371265769005, 0.09622462093830109, 0.038134876638650894, 0.031135767698287964, -0.0758492648601532, 0.017818808555603027, -0.05012990161776543, 0.007719798944890499, -0.04694923013448715, -0.029028410091996193, 0.020347613841295242, 0.0519399493932724, 0.020630426704883575, -0.03577140346169472, 0.026522178202867508, 0.038080524653196335, -0.051192525774240494, -0.06872225552797318, -0.03543980047106743, 0.08038211613893509, -0.03772076219320297, -0.0019484650110825896, 0.037762049585580826, -0.10211655497550964, 0.04411964863538742, -0.07546437531709671, -0.040720872581005096, -0.01922151818871498, -0.02725924737751484, -0.08222347497940063, 0.003876697737723589, -0.061851996928453445, -0.031574055552482605, 0.024813687428832054, 0.004459897987544537, -0.06730470806360245, 0.07732278108596802, 2.7396774800164803e-33, -0.07120928168296814, 0.09144872426986694, -0.0604870580136776, 0.09686953574419022, 0.08657430112361908, -0.02067793533205986, 0.12994472682476044, 0.007910454645752907, -0.08014624565839767, 0.1802346408367157, -0.0012835395755246282, 0.03514110669493675, 0.042759232223033905, -0.032923728227615356, 0.03707115724682808, 0.015503033995628357, 0.06208204850554466, -0.0637759417295456, -0.00826842151582241, -0.031668417155742645, -0.09215668588876724, 0.12675628066062927, 0.05259893462061882, 0.06083960831165314, -0.06681504845619202, 0.041024234145879745, 0.023306608200073242, -0.0696190670132637, -0.0015632979338988662, -0.018594680353999138, 0.015709752216935158, 0.0037315862718969584, -0.06175531446933746, 0.006468655075877905, 0.031019357964396477, -0.013525241985917091, 0.1241416186094284, -0.03099038079380989, -0.053906723856925964, 0.08038649708032608, 0.014954712241888046, 0.11118161678314209, 0.11494535952806473, 0.10129859298467636, -0.008285170421004295, 0.018935466185212135, 0.018787886947393417, -0.09819530695676804, 0.021266572177410126, 0.055501148104667664, -0.06605244427919388, -0.00862936582416296, 0.026670807972550392, 0.06042525917291641, -0.042968593537807465, 0.016999373212456703, -0.0363522469997406, -0.0009375516674481332, 0.024538744240999222, 0.01933225430548191, -0.08255282789468765, 0.07356218248605728, -0.03603252395987511, 0.04258878529071808, -0.0128817493095994, -0.022433863952755928, -0.07963509112596512, 0.07690770924091339, 0.040927913039922714, 0.010309535078704357, 0.0663880705833435, 0.03659835457801819, -0.13319484889507294, -0.05818798765540123, 0.06462288647890091, -0.09328170120716095, -0.04428894445300102, 0.006638913415372372, 0.02975262701511383, -0.043026696890592575, -0.04989458993077278, -0.07609459012746811, 0.003996340092271566, 0.034756213426589966, -0.07185260951519012, 0.09365355968475342, -0.02969425544142723, 0.032247986644506454, -0.04721519351005554, 0.026805879548192024, -0.0013494578888639808, -3.35526347043924e-05, -0.009545459412038326, -0.049362294375896454, -0.015678929165005684, -1.6930899349176798e-08, -0.009994355030357838, -0.049812640994787216, -0.005801562685519457, 0.011708877980709076, -0.030966216698288918, 0.07448813319206238, 0.04264145717024803, -0.053642142564058304, -0.05357576534152031, 0.005138280801475048, 0.09430532902479172, 0.0592377744615078, -0.06483539938926697, 0.047453366219997406, 0.0664837434887886, -0.08378928154706955, 0.013958822935819626, 0.001708249212242663, -0.010309797711670399, 0.07282484322786331, -0.09265510737895966, 0.029341895133256912, 0.030910231173038483, 0.040054284036159515, -0.01109660230576992, 0.056348562240600586, 0.025945425033569336, 0.08576096594333649, 0.003964757081121206, 0.024685867130756378, 0.055565573275089264, 0.06292921304702759, -0.040880974382162094, -0.05910543352365494, -0.02184230647981167, 0.062095899134874344, 0.03349875286221504, -0.03533301129937172, 0.0267812330275774, 0.01788383349776268, -0.10489799827337265, -0.005059277638792992, 0.0010323041351512074, 0.04416537284851074, -0.05984349548816681, -0.04777807369828224, -0.07838761806488037, -0.02046036347746849, -0.030605660751461983, -0.05824664235115051, -0.03258360177278519, -0.005576286930590868, 0.04223371669650078, -0.020661145448684692, 0.011428107507526875, -0.04831383749842644, -0.036218300461769104, -0.010730714537203312, -0.08764959871768951, 0.036847177892923355, 0.11295994371175766, -0.016820374876260757, 0.09417641907930374, -0.04484279081225395]\n</code></pre> <p>It is also possible to use a GPU for inference:</p> <pre><code>from bookacle.models.embedding import SentenceTransformerEmbeddingModel\n\nembedding_model = SentenceTransformerEmbeddingModel(model_name=\"all-MiniLM-L6-v2\", use_gpu=True)\ntext = \"This is a test\"\nembeddings = embedding_model.embed(text)\nprint(embeddings)\n</code></pre> <pre><code>[0.030612383037805557, 0.013831347227096558, -0.020843833684921265, 0.01632789708673954, -0.010231463238596916, -0.047984249889850616, -0.017313417047262192, 0.03728742152452469, 0.045887384563684464, 0.034405048936605453, -0.019959749653935432, -0.04465903341770172, -0.013102812692523003, 0.042841196060180664, -0.055393286049366, -0.058980002999305725, 0.013357766903936863, -0.040939509868621826, -0.046640221029520035, 0.030635859817266464, 0.03436749055981636, 0.06017487496137619, -0.05983395129442215, 0.017685670405626297, 0.006318103987723589, -0.011531681753695011, -0.05604176223278046, 0.023062868043780327, 0.035522881895303726, -0.0007312601082958281, -0.004532931372523308, 0.05712536349892616, 0.06493884325027466, 0.02289614826440811, 0.03908286988735199, 0.015843091532588005, 0.07268384099006653, 0.047734085470438004, 0.008836453780531883, 0.038440458476543427, 0.017816348001360893, -0.09784718602895737, 0.01985286921262741, 0.026004822924733162, 0.004592714831233025, 0.05503907799720764, -0.04667204990983009, 0.035593658685684204, -0.061679404228925705, 0.0011078142561018467, -0.016140861436724663, -0.026405761018395424, -0.07949257642030716, -0.08490551263093948, 0.02612096257507801, -0.00035369035322219133, 0.031159967184066772, -0.029759922996163368, 0.07323474436998367, 0.05357593670487404, -0.015631062909960747, -0.016084613278508186, -0.02865147963166237, 0.03595295920968056, 0.09541893005371094, 0.027765469625592232, -0.04436487331986427, -0.08868948370218277, -0.01265368890017271, -0.05754895135760307, 0.0002944806474260986, 0.011904970742762089, 0.02711832895874977, 0.04274006932973862, 0.029987165704369545, 0.013047569431364536, -0.03863208740949631, -0.09747391939163208, 0.06669679284095764, 0.040187254548072815, -0.09090323001146317, -0.06058502197265625, -0.011495315469801426, 0.02387797273695469, -0.011324175633490086, 0.06687263399362564, 0.08630166202783585, -0.025463048368692398, -0.0988529622554779, -0.006116422824561596, -0.011836516670882702, 0.05598629638552666, -0.060939811170101166, -0.012464525178074837, 0.016563139855861664, -0.021560898050665855, -0.02712395414710045, -0.002014313591644168, -0.0597769170999527, 0.10486981272697449, -0.002733857138082385, 0.013900969177484512, -0.0116738211363554, 0.01303277350962162, -0.07930489629507065, -0.07446799427270889, 0.03511616215109825, -0.04340587556362152, 0.06082131713628769, 0.00016751731163822114, -0.0566425696015358, -0.02639148384332657, 0.06102263182401657, 0.05601632967591286, 0.01758638210594654, -0.020373661071062088, -0.11545398086309433, 0.04475242272019386, 0.029722386971116066, 0.033025093376636505, 0.0817747414112091, -0.007757122162729502, -0.0009871120564639568, -0.03362540528178215, 0.013657217845320702, -0.022546373307704926, 0.01930450275540352, -6.218768457349027e-33, -0.012873685918748379, -0.04175008088350296, 0.025677861645817757, 0.07849736511707306, -0.016293944790959358, 0.01920202001929283, -0.035307660698890686, 0.04852418601512909, -0.010347968898713589, -0.021346205845475197, -0.02838769368827343, -0.07118812203407288, -0.001110539073124528, 0.016433581709861755, 0.08224152773618698, 0.11024080216884613, -0.013450088910758495, 0.10640895366668701, -0.07292727380990982, 0.061268869787454605, -0.05524792894721031, 0.031974975019693375, 0.0012061159359291196, -0.10626249760389328, -0.08820132166147232, -0.053465452045202255, -0.01210002601146698, -0.0006844076560810208, -0.005452686920762062, 0.02633832022547722, -0.012515145353972912, 0.039600953459739685, -0.07330328971147537, 0.03817976638674736, -0.02600649558007717, -0.022073546424508095, 0.012753630988299847, -0.01975107565522194, -0.011463160626590252, 0.014134258963167667, -0.06021532788872719, -0.04115568846464157, -0.03330481797456741, 0.05939905345439911, 0.06408755481243134, -0.028469711542129517, -0.02599479816854, -0.03844071552157402, 0.07594474405050278, 0.0013290699571371078, -0.0063445172272622585, -0.0017964555881917477, -0.002401858801022172, -0.012808689847588539, -0.04043188318610191, 0.05020056292414665, 0.0389358252286911, 0.01759772002696991, -0.04384368658065796, 0.09622456878423691, 0.038134850561618805, 0.031135717406868935, -0.07584924250841141, 0.01781882531940937, -0.05012987554073334, 0.007719795219600201, -0.04694923385977745, -0.0290283914655447, 0.020347578451037407, 0.051939938217401505, 0.020630445331335068, -0.03577139973640442, 0.02652217447757721, 0.038080524653196335, -0.051192548125982285, -0.06872230768203735, -0.035439833998680115, 0.08038201928138733, -0.037720780819654465, -0.0019484470831230283, 0.037762079387903214, -0.10211650282144547, 0.04411967843770981, -0.0754644051194191, -0.040720898658037186, -0.019221575930714607, -0.027259254828095436, -0.08222354203462601, 0.0038767221849411726, -0.061852067708969116, -0.03157401457428932, 0.024813655763864517, 0.004459884017705917, -0.06730472296476364, 0.077322818338871, 2.739677296345488e-33, -0.07120931148529053, 0.09144869446754456, -0.06048703193664551, 0.09686953574419022, 0.08657427132129669, -0.020677955821156502, 0.12994474172592163, 0.007910485379397869, -0.08014624565839767, 0.18023458123207092, -0.0012835804373025894, 0.03514113277196884, 0.042759209871292114, -0.032923705875873566, 0.03707120567560196, 0.015503027476370335, 0.06208207830786705, -0.0637759193778038, -0.008268370293080807, -0.03166833892464638, -0.09215668588876724, 0.1267562359571457, 0.052599016577005386, 0.06083954870700836, -0.06681501865386963, 0.041024208068847656, 0.0233065914362669, -0.0696190595626831, -0.001563273137435317, -0.01859469525516033, 0.01570979878306389, 0.0037316177040338516, -0.06175530329346657, 0.0064686862751841545, 0.031019343063235283, -0.013525290414690971, 0.12414152920246124, -0.03099030628800392, -0.053906671702861786, 0.08038657158613205, 0.014954760670661926, 0.1111815795302391, 0.11494539678096771, 0.10129859298467636, -0.008285125717520714, 0.01893540471792221, 0.01878790743649006, -0.09819523990154266, 0.0212665144354105, 0.05550113692879677, -0.06605248153209686, -0.00862941239029169, 0.02667074464261532, 0.06042523682117462, -0.042968638241291046, 0.01699935831129551, -0.036352287977933884, -0.0009375561494380236, 0.024538755416870117, 0.01933225803077221, -0.08255283534526825, 0.07356221973896027, -0.0360325463116169, 0.04258875921368599, -0.012881777249276638, -0.022433888167142868, -0.0796351507306099, 0.0769076719880104, 0.0409279502928257, 0.010309499688446522, 0.0663880705833435, 0.03659844398498535, -0.13319484889507294, -0.05818799138069153, 0.06462284922599792, -0.09328165650367737, -0.04428890347480774, 0.006638932507485151, 0.02975265122950077, -0.04302671179175377, -0.04989457502961159, -0.07609450817108154, 0.003996350336819887, 0.03475615382194519, -0.0718526542186737, 0.0936535969376564, -0.029694192111492157, 0.032247912138700485, -0.047215137630701065, 0.026805812492966652, -0.001349434140138328, -3.352389467181638e-05, -0.009545411914587021, -0.04936238378286362, -0.01567898318171501, -1.693089757281996e-08, -0.009994382038712502, -0.04981253296136856, -0.005801553837954998, 0.011708883568644524, -0.030966246500611305, 0.07448814809322357, 0.04264155402779579, -0.05364212021231651, -0.05357576161623001, 0.005138292443007231, 0.09430535137653351, 0.0592377744615078, -0.06483540683984756, 0.047453366219997406, 0.06648370623588562, -0.08378919214010239, 0.013958729803562164, 0.0017082258127629757, -0.01030980795621872, 0.0728248804807663, -0.09265513718128204, 0.029341932386159897, 0.03091024048626423, 0.04005422815680504, -0.01109654176980257, 0.056348491460084915, 0.025945376604795456, 0.0857609286904335, 0.003964828327298164, 0.024685846641659737, 0.055565591901540756, 0.06292914599180222, -0.0408809557557106, -0.05910545587539673, -0.021842265501618385, 0.062095873057842255, 0.03349878266453743, -0.03533300757408142, 0.02678123116493225, 0.017883868888020515, -0.10489799827337265, -0.005059306044131517, 0.0010322809685021639, 0.04416538029909134, -0.0598435215651989, -0.04777806997299194, -0.07838766276836395, -0.02046039327979088, -0.03060564212501049, -0.05824664607644081, -0.03258366882801056, -0.005576316732913256, 0.04223372042179108, -0.020661158487200737, 0.011428066529333591, -0.048313844949007034, -0.03621833398938179, -0.010730783455073833, -0.08764956891536713, 0.03684716671705246, 0.11295989155769348, -0.0168203953653574, 0.09417645633220673, -0.04484285041689873]\n</code></pre>"},{"location":"usage/models/#use-a-summarization-model-from-huggingface","title":"Use a summarization model from HuggingFace","text":""}]}